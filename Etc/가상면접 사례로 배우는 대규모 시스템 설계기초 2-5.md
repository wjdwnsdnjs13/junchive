# 9장 - S3와 유사한 객체 저장소
- S3는 AWS가 제공하는 RESTful API 기반 인터페이스로 사용하는 객체 저장소 서비스임
- 버전 관리, 버킷 정책, 멀티파트 업로드, 서버 측 암호화, 여러 객체 삭제, 객체 만료 등을 지원
- 수명 주기 정책, 이벤트 알림, 지역 간 복제 등의 기능 도입
- 2021년 아마존은 S3에 저장된 객체가 100조 개가 넘는다고 함

## 저장소 시스템 101
- 저장소 시스템의 3가지 부류
  - 블록 저장소
  - 파일 저장소
  - 객체 저장소

### 블록 저장소
- 1960년대 처음 등장
- HDD, SSD처럼 서버에 물리적으로 연결되는 형태의 드라이브가 가장 흔한 형태임
- 서버는 원시 블록을 포맷한 다음 파일 시스템을 이용하거나 가상 머신 엔진 같은 애플리케이션에 블록 제어권을 넘겨버릴 수도 있음
- DB나 가상 머신 엔진 같은 애플리케이션은 원시 블록을 직접 제어해 최대한의 성능을 끌어 냄
- 물리적으로 직접 연결되는 저장소에 국한되지 않음
- 고속 네트워크나 FC(Fibre Channel)이나 iSCSI를 통해 연결될 수도 있음
- 네트워크를 통해 연결되는 블록 저장소도 원시 블록을 제공한다는 점에서 다르지 않음
  - 서버 입장에서는 물리적으로 연결된 블록 저장소와 동일하게 동작 함

### 파일 저장소
- 파일 저장소는 블록 저장소 위에 구현됨
- 파일과 디렉토리를 다루기 쉽게하는 더 높은 수준의 추상화를 제공함
- 데이터는 계층적으로 구성되는 디렉토리 안에 보관됨
- 파일 저장소는 가장 널리 사용되는 범용 저장소 솔루션임
- SMB/CIFS나 NFS 같은 파일 수준 네트워크 프로토콜 사용 시 하나의 저장소에 여러 서버를 동시에 붙일 수도 있음
- 서버는 블록을 직접 제어하고, 포맷하는 등의 작업을 신경 쓸 필요가 없음

### 객체 저장소
- 새로운 형태의 저장소임
- 데이터 영속성을 높이고 대규모 애플리케이션을 지원하고 비용을 낮추기 위해 의도적으로 성능을 희생함
- 실시간 갱신이 필요가 없는 상대적으로 '차가운(cold)' 데이터 보관에 초점을 맞춰 아카이브나 백업에 주로 쓰임
- 모든 데이터를 수평적 구조 내에 객체로 보관함
- 계층적 디렉토리 구조는 제공하지 않음
- RESTful API를 통해 객체를 저장하고 검색함
- 다른 유형의 저장소에 비해 상대적으로 느림

|--| 블록 저장소 | 파일 저장소                  | 객체 저장소 |
|--|--|-------------------------|--|
|저장된 내용의 변경 가능성 | Y | Y                       | N(직접 변경은 불가, 객체 버전을 통해 새로운 버전의 객체 추가는 가능) |
| 비용 | 고 | 중 ~ 고                   | 저 |
| 성능 | 중 ~ 고 혹은 최상 | 중 ~ 고                   | 저 ~ 중 |
| 데이터 일관성 | 강력 | 강력                      | 강력 |
| 데이터 접근 | SAS/iSCSI/FC | 표준 파일 접근, CIFS/SMB, NFS | RESTful API |
| 규모 확장성 | 중 | 고                       | 최상 |
| 적합한 응용 | VM, DB같은 높은 성능이 필요한 애플리케이션 | 범용적 파일 시스템 접근           | 이진 데이터, 구조화되지 않은 데이터 |

### 용어 정리
- S3 설계를 위해서는 핵심 개념이해가 필요함
- 버킷
  - 객체를 보관하는 논리적 컨테이너
  - 버킷 이름은 전역적으로 유일해야 함
  - S3에 데이터 업로드를 위해서는 버킷부터 만들어야 함
- 객체
  - 객체는 버킷에 저장하는 개별 데이터를 의미
    - 데이터(페이로드라고도 함)와 메타데이터를 가짐
    - 데이터는 어떤 것도 가능
  - 메타 데이터는 객체를 기술하는 이름-값 쌍의 집합
- 버전
  - 한 객체의 여러 버전을 같은 버킷에 저장하게 하는 기능
  - 버킷마다 별도 설정 가능
  - 실수로 지우거나 덮어쓴 객체를 복구할 수 있게 함
- URI(Uniform Resource Identifier)
  - 객체 저장소는 버킷과 객체에 접근할 수 있는 RESTful API를 제공함
  - 각 객체는 해당 API URI를 통해 고유하게 식별 가능
- SLA(Service-Level Agreement)
  - 서비스 수준 협약(SLA)는 서비스 제공자와 클라이언트 사이 맺어지는 계약임
  - S3 Standard-LA는 다음 SLA를 만족함
    - 여러 가용성 구역에 걸쳐 99.999999999%의 객체 내구성을 제공하도록 설계함
    - 하나의 가용성 구역 전체가 소실되어도 데이터 복원 가능
    - 연 간 99.9%의 가용성 제공

## 1단계 - 문제 이해 및 설계 범위 확정
- 요구 사항 분석
  - Q) 어떤 기능을 지원하는가?
    - A) 다음 기능을 제공하는 S3와 유사한 객체 저장소 시스템을 설계하고자 함
    - 버킷 생성
    - 객체 업로드 및 다운로드
    - 객체 버전
    - 버킷 내 객체 목록 출력 기능.
      - aws s3 ls 명령어와 유사
  - Q) 데이터의 크기는
    - A) 수 GB 이상 아주 큰 객체와 수 KB 정도의 다량의 소형 객체를 효율적으로 저장할 수 있어야 함 
  - Q) 매년 추가되는 데이터는 어느 정도인가?
    - A) 100PB
  - Q) 99.9999%의 데이터 내구성과 99.99%의 서비스 가용성을 보장한다고 하면 되는가?
    - A) 얍얍

### 비기능 요구 사항
- 100PB 데이터
- 식스 나인(six nines, 99.9999%) 수준의 데이터 내구성
- 포 나인(four nines, 99.99%) 수준의 서비스 가용성
- 저장 효율성
  - 높은 수준의 안정성과 성능은 보증하되 저장소 비용은 최대한 낮추어야 함

### 대략적인 규모 추정
- 객체 저장소는 디스크 용량이나 초당 디스크 IO(IOPS)가 병목이 될 가능성이 높음
  - 디스크 용량(가정)
    - 객체 가운데 20%는 크기가 1MB 미만의 작은 객체
    - 60% 정도의 객체는 1MB ~ 64MB 정도 크기의 중간 크기 객체다
    - 나머지 20% 정도는 64MB 이상의 대형 객체다
  - IOPS
    - SATA 인터페이스를 탑재하고 7200rpm을 지원하는 하드 디스크 하나가 초당 100~150회의 임의 데이터 탐색을 지원한다고 가정(100 ~ 150 IOPS)
- 계산을 쉽게하기 위해 중앙값을 사용
  - 소형 객체는 0.5MB
  - 중형 객체는 32MB
  - 대형 객체는 200MB
  - 40% 저장 공간 사용률을 유지하는 경우 저장소에 수용 가능한 객체의 수는 다음과 같음
    - 100PB = 100 * 1000 * 1000 * 1000MB = 10MB
    - 6억 8천만 개의 객체
      - 10^11 * 0.4 / (0.2 * 0.5MB + 0.6 * 32MB + 0.2 * 200MB)
    - 모든 객체의 메타데이터 크기가 대략 1KB 정도라고 가정하면 모든 메타데이터 정보를 저장하기 위해 0.68TB 정도의 공간이 필요함

## 2단계 - 개략적 설계안 제시 및 동의 구하기
- 알고 가면 좋은 객체 저장소의 몇 가지 속성
  - 객체 불변성
    - 객체 저장소와 다른 두 가지 유형의 저장소와 가장 큰 차이는 객체 저장소에 보관되는 객체는 변경이 불가능하다는 것임
    - 삭제한 다음 새 버전 객체로 대체할 수는 있어도 그 값을 점진적으로 변경할 수 없음
  - key-value 저장소
    - 객체 저장소를 사용하는 경우 해당 객체의 URI를 사용해 데이터를 가져올 수 있음
    - URI는 키, 데이터는 값이므로 key-value 저장소라고 볼 수 있음
  - 저장은 1회 읽기는 여러번
    - 데이터 접근 패턴 측명에서 보면 쓰기는 1회, 읽기는 여러 번 발생함
    - 링크드인의 조사에 따르면 객체 저장소에 대한 요청 가운데 95% 정도가 읽기 요청임
  - 소형 및 대형 객체 동시 지원
    - 다양한 크기의 객체를 문제 없이 저장할 수 있음
- 객체 저장소의 설계 철학은 UNIX 파일 시스템의 설계 철학과 아주 비슷함
- UNIX는 파일을 로컬 파일 시스템에 저장하면 파일의 이름과 데이터는 같은 곳에 저장되지 않음
  - 파일 이름을 아이노드라고 불리는 자료 구조에 저장하고 파일의 데이터는 디스크의 다른 위치에 들어감
  - 아이노드에는 파일의 데이터가 실제로 보관되는 디스크 상의 위치를 가리키는 파일 블록 포인터 목록이 저장됨
  - 따라서 파일을 읽을 때 우선 아이노드에 기록된 메타데이터를 읽어서 파일 블록 포인터 목록을 확보한 이후 그 포인터를 일일이 따라가 데이터를 읽어야 함
- 객체 저장소의 동작 방식도 비슷함
  - 메타데이터 저장소는 아이노드에 해당하고, 객체 데이터가 저장되는 데이터 저장소는 하드 디스크에 해당함
  - 다만 파일 블록포인터 대신 네트워크를 통해 데이터 저장소에 보관된 객체를 요청하는 데 필요한 식별자(ID)가 보관됨
- 메타데이터와 객체의 실제 데이터를 분리하면 설계가 단순해짐
  - 데이터 저장소에 보관되는 데이터는 불변이고, 메타데이터는 변경이 가능함
  - 분리하면 두 컴포넌트는 독립적으로 구현하고 최적화할 수 있음

### 개략적 설계안
- 중요 컴포넌트
  - 로드밸런서
    - RESTful API에 대한 요청을 API 서버들에 분산하는 역할
  - API 서비스 
    - IAM 서비스, 메타데이터 서비스, 저장소 서비스에 대한 호출을 조율하는 역할을 담당
    - 무상태 서비스라서 수평적 확장 가능
  - IAM 서비스
    - 인증(authentication), 권한 부여(authorization), 접근 제어 등을 중앙에서 맡아서 처리
  - 데이터 저장소
    - 실제 데이터를 보관하고 필요할 때마다 읽어가는 저장소.
    - 모든 데이터 관련 연산은 객체 ID(UUID)를 통함
  - 메타데이터 저장소
    - 객체 메타데이터를 보관하는 장소
  - 메타데이터 저장소와 데이터 저장소는 논리적인 구분일 뿐이며 구현 방법은 여러 가지가 있을 수 있음
  - 가장 중요한 작업 흐름
    - 객체 업로드
    - 객체 다운로드
    - 객체 버전 및 버킷 내 모든 객체 목록 출력

### 객체 업로드
- 객체는 버킷 안에 두어야 함
- 총 7단계
  - 클라이언트는 bucket-to-share 버킷을 생성하기 위한 HTTP PUT 요청을 보내고, API 서비스로 전달됨
  - API 서비스는 IAM을 호출해 해당 사용자가 WRITE 권한을 가졌는지 확인함
  - API 서비스는 메타데이터 DB에 버킷 정보를 등록하기 위한 메타 데이터 저장소를 호출함
    - 버킷 정보가 만들어지면 그 사실을 알리는 메시지가 클라이언트에 전송됨
  - 버킷이 만들어진 후 클라이언트는 script.txt 객체를 생성하기 위한 HTTP PUT 요청을 보냄
  - API 서비스는 해당 사용자 신원 및 WRITE 권한 소유 여부를 확인
  - 확인 결과 문제 없으면 API 서비스는 HTTP PUT 요청 body에 실린 객체 데이터를 데이터 저장소로 보냄
    - 데이터 저장소는 해당 객체로 저장하고 해당 객체의 UUID를 반환함
  - API 서비스는 메타데이터를 호출해서 새로운 항목을 등록.
    - object_id(UUID), bucket_id(해당 객체가 속한 버킷), object_name 등의 정보가 포함됨

### 객체 다운로드
- 디렉토리 같은 계층 구조를 지원하지 않음
- 버킷 이름과 객체 이름을 연결하면 폴더 구조를 흉내 내는 논리적 계층을 만들 수는 있음
- 데이터 저장소는 객체 이름을 보관하지 않고 object_id(UUID)를 통한 객체 연산만 지원함
  - 객체를 다운로드하려면 객체 이름을 UUID로 변환해야 함
- 객체 다운로드 프로세스
  - 클라이언트는 GET /bucket-to-share/script.txt 요청을 로드밸런서로 보냄.
    - 로드밸런서는 이 요청을 API 서버로 보냄
  - API 서비스는 LAM을 질의해 사용자가 해당 버킷에 READ 권한을 갖고 있는지 확인
  - 권한이 있음을 확인하면 API 서비스는 해당 객체의 UUID를 메타데이터 저장소에서 가져옴
  - API 서비스는 해당 UUID를 사용해 데이터 저장소에서 객체 데이터를 가져 옴
  - API 서비스는 HTTP GET 요청에 대한 응답으로 해당 객체 데이터를 반환함

## 3단계 - 세부 설계
### 데이터 저장소
- API 서비스는 사용자 요청을 받으면 그 요청을 처리하기 위해 다른 내부 서비스들을 호출함
- 객체를 저장하거나 가져오는 작업은 데이터 저장소를 호출해 처리함

### 데이터 라우팅 서비스
- 라우팅 서비스는 데이터 노드 클러스터에 접근하기 위한 RESTful API 혹은 gRPC 서비스를 제공함
- 무상태 서비스
- 역할
  - 배치 서비스(placement service)를 호출해 데이터를 저장할 최적의 데이터 노드를 판단
  - 데이터 노드에서 데이터를 읽어 API 서비스에 반환
  - 데이터 노드에서 데이터 기록

### 배치 서비스
- 어느  데이터 노드에 데이터를 저장할지 결정하는 역할을 담당
- 주 데이터 노드와 부 데이터 노드가 있음
- 내부적으로 가상 클러스터 지도(virtual cluster map)를 유지함
- 지도에는 클러스터의 물리적 형상 정보가 보관됨
- 배치 서비스는 이 지도에 보관되는 데이터 노드의 위치 정보를 이용해 데이터 사본(replica)이 물리적으로 다른 위치에 놓이도록 함
- 물리적인 분리는 높은 데이터 내구성을 달성하는 핵심 요소임
- 배치 서비스는 모든 데이터 노드와 지속적으로 박동 메시지를 주고받으며 상태를 모니터링 함
- 15초의 유예기간(grace period) 동안 박동 메시지에 응답하지 않을 경우 죽은 노드로 표시함
- 배치 서비스는 아주 중요해서 5개 ~ 7개의 노드를 갖는 배치 서비스 클러스터를 팩서스나 래프트 같은 합의 프로토콜(consensus protocol)을 사용해 구축할 것을 권장함
  - 합의 프로토콜은 일부 노드에 장애가 생겨도 절반 이상이 건강하면 서비스를 지속하게 보장함

### 데이터 노드
- 실제 객체 데이터가 보관되는 곳
- 다중화 그룹
  - 여러 노드에 데이터를 복제해 데이터의 안정성과 내구성을 보증함.
- 각 데이터 노드에는 배치 서비스에 주기적으로 박동 메시지를 보내는 서비스 데몬이 있음
  - 해당 데이터 노드에 부착된 디스크 드라이브(HDD/SSD)의 수
  - 각 드라이브에 저장된 데이터의 양
- 배치 서비스는 못 보던 데이터 노드에서 박동 메시지를 처음 받으면 해당 노드에 ID를 부여하고 가상 클러스터 지도에 추가한 후 아래 정보를 반환함
  - 해당 데이터 노드에 부여한 고유 식별자
  - 가상 클러스터 지도
  - 데이터 사본을 보관할 위치

### 데이터 저장 흐름
- 데이터 노드에 데이터가 영속적으로 보관되는 흐름
  - API 서비스는 객체 데이터를 데이터 저장소로 포워딩함
  - 데이터 라우팅 서비스는 해당 객체에 UUID를 할당하고 배치 서비스에 해당 객체를 보관할 데이터 노드를 질의함
    - 배치 서비스는 가상 클러스터 지도를 참조해 해당 객체를 보관할 주 데이터 노드를 반환함
  - 데이터 라우팅 서비스는 저장할 데이터를 UUID와 함께 주 데이터 노드에 직접 전송함
  - 주 데이터 노드는 데이터를 자기 노드에 지역적으로 저장하고, 두 개의 부 데이터 노드에 다중화함.
    - 주 데이터 노드는 데이터를 모든 부 데이터 노드에 성공적으로 다중화한 후에는 데이터 라우팅 서비스에 응답을 보냄
  - 객체의 UUID(객체의 ID)를 API 서비스에 반환함
- 2 단계는 배치 서비스에 UUID를 입력으로 주고 질의하면 해당 객체에 대한 다중화 그룹이 반환된다는 뜻임
  - 계산 결과는 결정적(deterministic)이어야하고, 다중화 그룹이 추가되거나 삭제되는 경우에도 유지되어야 함
  - 이런 종류의 조회 연산 구현에는 보통 안정 해시(consistent hash)를 사용함
- 4단계는 응답을 반환하기 전 데이터를 모든 부 노드에 다중화한다는 뜻임
  - 모든 데이터 노드에 강력한 일관성이 보장됨
  - 가장 느린 사본에 작업이 완료될 때까지 응답이 반환되지 못해 지연 시간 측면에서 손해임

![img.png](img.png)
- 첫 선택지는 데이터를 세 노드에 전부 보관하면 성공적으로 보관했다고 간주함
  - 데이터 일관성에서는 최선이지만, 응답 지연이 가장 높음
- 두 번째는 데이터를 주 데이터 및 두 개의 부 노드 가운데 하나에 성공적으로 보관하면 성공이라고 간주함
  - 중간 정도의 데이터 일관성 및 응답 지연을 제공함
- 세 번째는 데이터를 주 데이터에 보관하면 성공이라고 간주함
  - 데이터 일관성 측면에서는 최악이지만 응답 지연은 가장 낮음

### 데이터는 어떻게 저장되는가
- 데이터 노드가 실제로 데이터를 어떻게 관리하는가?
  - 가장 단순한 방법은 각각의 객체를 개별 파일로 저장하는 것임
  - 잘 동작하지만, 작은 파일이 많아지면 성능이 떨어짐
  - 작은 파일이 많아질 경우 문제점
    - 낭비되는 데이터 블록 수가 늘어남
      - 파일 시스템은 파일을 별도 디스크 블록으로 저장함
      - 디스크 블록의 크기는 전부 같고 볼륨 초기화 시 결정되는데, 보통 4KB임
      - 4KB보다 작은 파일을 저장할 때에도 블록 하나를 온전히 씀
      - 따라서 작은 파일이 많아지면 낭비되는 블록이 늘어남
    - 아이노드 용량 한계를 초과하는 문제
      - 파일 시스템은 파일 위치 등의 정보를 아이노드라는 특별한 유형의 블록에 저장함
      - 대부분의 파일 시스템의 경우 사용 가능한 아이노드 수는 디스크 초기화 순간 결정됨
      - 작은 파일 수가 수백만개에 달하면 아이노드가 전부 소진될 수 있음.
      - 운영 체제는 파일 시스템 메타데이터를 공격적으로 캐싱하는 전략을 취해도 아주 많은 양의 아이노드를 효과적으로 처리하지 못 함
      - 따라서 작은 객체를 개별 파일 형태로 저장하는 방안은 현실에서 쓸모가 없음
  - 이런 문제는 작은 객체들을 큰 파일 하나로 모아서 해결 가능함
  - 개념적으로 WAL(Write Ahead Log)와 같이 이미 존재하는 파일에 추가하는 방식임
    - WAL은 4장 "분산 메시지 큐"를 참고하기
  - 용량 임계치에 도달한 파일(보통 수 GB)은 읽기 전용 파일로 변경하고 새로운 파일을 만듦
  - 읽기-쓰기 파일에 대한 쓰기 연산은 순차적으로 이루어져야 한다.

### 객체 소재 확인
- 각 데이터 파일 안에 많은 작은 객체가 들어있을 때 UUID로 객체 위치를 찾는 방법
  - 객체가 보관된 데이터 파일
  - [데이터 파일 내 객체 오프셋(offset)](#q-uuid로-객체-위치-찾을-때-오프셋이-필요한-이유는-무엇이고-오프셋은-무엇인가)
  - 객체 크기
- 위의 데이터들을 가진 DB 스키마는 RocksDB같은 파일 기반 key-value 저장소를 사용하는 방법과 RDB를 사용하는 방법이 있다.
  - RocksDB는 SSTable에 기반한 방법으로 쓰기는 좋지만 읽기가 느리다.
  - RDB는 B+ 트리 기반으로 저장하며 읽기는 좋지만 쓰기는 느리다.
- 객체 위치를 저장하는 테이블의 데이터 양은 막대함
  - 데이터 노드에 저장되는 위치 데이터는 다른 데이터 노드와 공유할 필요가 없음
    - 각 데이터 노드마다 RDB를 설치하는 방법도 가능함
    - SQLite 같은 경량 DB를 사용하면 좋음

### 개선된 데이터 저장 흐름
- API 서비스는 새로운 객체를 저장하는 요청을 데이터 노드 서비스에 전송함
- 데이터 노드 서비스는 해당 객체를 읽기-쓰기 파일 /data/c의 마지막 부분에 추가함
- 해당 객체에 대한 새로운 레코드를 object_mapping 테이블에 추가함
- 데이터 노드 서비스는 API 서비스의 해당 객체의 UUID를 반환함

### 데이터 내구성
- six nines(99.9999%) 수준의 데이터 내구성을 보장하기 위해서는 장애가 발생할 모든 경우를 세심하게 살피고 여러 데이터를 적절히 다중화해야 한다.

### 데이터 다중화 - 하드웨어 장애와 장애 도메인
- 하드 디스크 장애는 피할 수 없음
- 드라이브 한 대로는 내구성 목표를 달성할 수 없을 가능성이 높음
- 데이터를 여러 대의 드라이브에 복제
  - 여기선 3중으로 복제함
  - 데이터 3중 복제 시 내구성은 약 0.999999이다.
- 장애 도메인은 서비스에 문제가 발생했을 때 부정적인 영향을 받는 물리적/논리적 구획을 말함
- 데이터 센터의 가용성 구역은 대표적인 대규모 장애 도메인 사례다.
  - 데이터를 여러 AZ에 복제해 장애 여파를 최소화 할 수 있음

### 데이터 다중화 - 소거 코드(erasure coding)
- 3중 데이터 다중화는 대략 99.9999%의 내구성을 달성
- 다른 방법 중 소거 코드가 있음
- 데이터를 작은 단위로 분할해 다른 서버에 배치하고, 일부가 소실되었을 때 복구하기 위해 패리티라는 정보를 만들어 중복성을 확보함
  - 4 + 2 소거 코드 사례
    - 데이터를 4개의 같은 크기 단위로 분할
    - 수학 공식을 사용해 패리티 p1, p2를 계산.
      - p1 = d1 + 2 * d2 - d3 + 4 * d4
      - p2 = -d1 + 5 * d3 - 3 * d4
    - - 데이터 d3와 d4가 노드 장애로 소실되었을 때
    - 남은 값 d1, d2, p1, p2와 페리티 계산에 쓰인 수식을 결합해 d3와 d4를 복원 가능함
  - 소거 코드를 사용하면 최대 8개의 건강한 노드에서 데이터를 가져와야할 수 있다.
    - 소거 코드를 추가하면 2개의 데이터 블록에 하나의 패리티 블록이 필요하므로 50% 오버헤드가 발생한다.
    - 3중화는 200% 오버헤드가 발생함

|--| 다중화 | 소거 코드                                                                      |
|--|--|----------------------------------------------------------------------------|
| 저장소 효율성 | 200%의 저장 용량 오버헤드 | 50%의 저장 용량 오버헤드.                                                           |
| 계산 자원 | 계산이 필요 없음 | 패리티 계산에 많은 계산 자원 소모                                                        |
| 쓰기 성능 | 데이터를 여러 노드에 복제. 추가로 필요한 계산 없음. | 데이터를 디스크에 기록하기 전 패리티 계산이 필요해 쓰기 연산의 응답 지연 증가                              |
| 읽기 성능 | 장애가 발생하지 않은 노드에서 데이터를 읽음 | 데이터를 읽어야 할 때마다 클러스터 내의 여러 노드에 데이터를 가져와야 함. 장애 발생 시 빠진 데이터 복원이 필요해 지연 시간 증가 |

- 응답 지연이 중요할 땐 다중화가, 저장소 비용이 중요할 땐 소거 코드가 유리함
- 소거 코드는 비용과 내구성 측면에서 매력적이지만 데이터 노드 설계에서 까다로움

### 데이터 다중화 - 정확성 검증
- 대규모 시스템은 디스크 훼손이 아니라 메모리의 데이터가 망가지는 일도 빈번하다
- 메모리 데이터 훼손은 프로세스 경계에 데이터 검증을 위한 체크섬을 두어 해결할 수 있다.
  - 새로 계산한 체크섬이 원본 체크섬과 다르면 데이터가 망가진 것
  - 같은 경우에는 아주 높은 확률로 데이터가 온전함
  - 같다고 온전하지 않을 수 있지만 아주 낮은 확률이기에 현실적으로 같다고 함
  - 체크섬에는 MD5, SHA1, HMAC 등 다양한 알고리즘이 존재함
- 체크섬과 (8 + 4) 소거 코드 사용 시 절차
  - 객체 데이터와 체크섬을 가져옴
  - 수신된 데이터의 체크섬을 계산
    - 두 체크섬이 일치하면 데이터에는 에러가 없다고 간주
    - 체크섬이 다르면 데이터가 망가진 것이므로 다른 장애 도메인에서 데이터를 가져와 복구를 시도
  - 데이터 8조각을 전부 수신할 때까지 1, 2를 반복해 원래 객체를 복원한 후 클라이언트에게 보냄

### 메타데이터 데이터 모델 - 스키마
- 지원해야하는 질의 3가지
  - 객체 이름으로 객체 ID 찾기
  - 객체 이름에 기반해 객체 삽입 또는 삭제
  - 같은 접두어를 갖는 버킷 내의 모든 객체 목록 확인

### 메타데이터 데이터 모델 - bucket 테이블의 규모 확장
- 보통 한 사용자가 만들 수 있는 버킷은 제한이 있어 이 테이블의 크기는 작음
- 100만명의 고객이 각자 10개의 버킷을 갖고 있고 한 레코드의 크기가 10KB라고 가정
  - 10GB(100만 * 10 * 10KB) = 100GB
  - 최신 DB 한 대에 충분히 저장 가능
  - 하지만, 모든 읽기 요청을 처리하기에는 CPU 용량이나 네트워크 대역폭이 부족할 수 있음
  - DB 사본을 만들어 읽기 부하 분산이 필요함

### object 테이블의 규모 확장
- object 테이블에는 객체 메타데이터를 보관함
- 객체 메타 데이터를 DB 서버 한 대에 보관할 수 없음
  - 샤딩을 통해 테이블 규모를 확장함
  - bucket_id를 기준으로 같은 버킷 내 객체는 같은 샤드에 배치되게 함
    - 3가지 질의 중 1, 2 질의를 효율적으로 지원하지 못 함
      - URI를 기준으로 하기 때문
  - bucket_name과 object_name을 결합해 샤딩
    - 대부분 메타데이터 관련 연산이 객체 URI를 기준으로 함
      - 객체 ID 검색도 객체 URI를 기준으로하고, 객체 업로드도 마찬가지임
    - bucket_name과 데이터를 균등하게 분산하기 위해서는 object_name의 순서쌍을 해싱한 값을 샤딩 키로 사용하면 됨
    - 이럴 경우 3번째 질의가 다소 애매할 수 있음

### 버킷 내 객체 목록 확인
- 객체 저장소는 파일 시스템처럼 계층적 구조로 보관하지 않음
- 객체는 s3://<버킷 이름>/<객체 이름>의 수평적 경로로 접근함
- s3는 사용자가 객체를 잘 정리할 수 있게 접두어를 지원함.
  - 접두어를 잘 사용하면 디렉토리와 비슷하게 데이터를 정리할 수 있음.
- S3가 제공하는 목록 출력 명령어는 다음과 같이 쓰임
  - 어떤 사용자가 가진 모든 버킷 목록 출력
    - aws s3 list-buckets
  - 주어진 접두어를 가진 같은 버킷 내 모든 객체 목록 출력
    - aws s3 ls s3://<버킷 이름>/<접두어>
  - 주어진 접두어를 가진, 같은 버킷 내 모든 객체를 재귀적으로 출력.
    - aws s3 ls s3://<버킷 이름>/<접두어> --recursive

### 단일 DB 서버
- 특정 사용자가 가진 모든 버킷 출력
  - SELECT * FROM bucket WHERE owner_id = {id}
- 같은 접두어를 갖는 버킷 내 모든 객체를 출력
  - SELECT * FROM object WHERE bucket_id = {bucket_id} AND object_name LIKE {prefix}%

### 분산 DB
- 메타데이터 테이블을 샤딩하면 어떤 샤드에 데이터가 있는지 몰라서 목록 출력 기능을 구현하기 어려움
  - 가장 단순한 방법은 검색 질의를 모든 샤드에 돌려 결과를 취합하는 것임
    - SELECT * FROM object WHERE bucket_id = {bucket_id} AND object_name LIKE {prefix}%
  - 메타 데이터 서비스는 각 샤드가 반환한 객체들을 취합해 그 결과를 호출해 클라이언트에 반환
    - 페이지 나눔 기능을 구현하기 복잡함
    - SELECT * FROM object WHERE bucket_id = {bucket_id} AND object_name LIKE {prefix}% ORDER BY object_name OFFSET 0 LIMIT 10
    - OFFSET과 LIMIT를 사용해 반환되는 결과를 객체의 수를 제한함
    - 이후 10개를 만들 땐 OFFSET을 10으로 설정해 질의함
    - 여기서 힌트는 서버가 클라이언트에 각 페이지를 보낼 때 붙여 보내는 커서(cursor)를 말함
    - 커서 안에는 오프셋 정보가 인코딩되어 있음.
      - 클라이언트는 다음 페이지 요청 시 커서를 요청에 실어 보내고, 서버는 커서를 디코딩해 얻은 오프셋 정보를 질의에 심어 다음 페이지를 가져옴
    - SELECT * FROM object WHERE bucket_id = {bucket_id} AND object_name LIKE {prefix}% ORDER BY object_name OFFSET 10 LIMIT 10
    - 페이지 나눔 기능을 구현하기 어려운 이유
      - 객체가 여러 샤드에 나뉘어 있어 샤드마다 반환하는 객체의 수가 제각각임
      - 어떤 샤드에는 10개의 객체가 있을 수 있지만, 그보다 적거나 아예 빈 샤드가 있을 수도 있음.
        - 그 중에서 모든 샤드의 결과를 받아 취합해 정렬해서 10게만 추려야 함
        - 그 말은 즉 샤드마다 추적해야하는 오프셋이 달라질 수 있다는 의미다.

### 객체 버전
- 버킷 안에 한 객체의 여러 버전을 둘 수 있게 하는 기능임
- 실수로 지우거나 덮어 쓴 객체를 쉽게 복구 가능
- 버전 기능 활성화 시 객체 저장소는 모든 이전 버전을 메타데이터 저장소에 유지하고, 이전 버전에 삭제 표시를 하지 않음
  - 클라이언트는 객체를 업로드하기 위한 HTTP PUT 요청을 보냄
  - API 서비스는 사용자의 신원을 확인하고 해당 사용자가 해당 버킷에 WRITE 권한을 갖고 있는지 확인함
  - 확인 결과 문제가 없으면 데이터를 데이터 저장소에 업로드함.
    - 데이터 저장소는 새 객체를 만들어 데이터를 영속적으로 저장하고 새로운 UUID를 반환함
  - API 서비스는 메타데이터 저장소를 호출해 새 객체의 메타데이터 정보를 보관함
  - 버전 기능을 지원하기 위해 메타데이터 저장소의 객체 테이블에 object_version이라는 이름의 열이 있음. 해당 열은 버전 기능 활성화 시에 사용됨
    - bucket_id와 object_name은 같지만, object_id와 object_version은 다른 값을 가짐
    - object_id는 이전 단계에서 반환된 UUID임
    - object_version은 TIMEUUID임
    - 메타데이터 저장소로 사용되는 DB는 특정 객체의 현재 버전을 조회하는 연산을 효과적으로 처리할 수 있어야 함
- 버전이 다른 파일 삭제하는 방법
  - 파일은 그대로 두고, 삭제 표식(delete marker)을 추가함

### 큰 파일의 업로드 성능 최적화
- 개략적 시스템 규모 추정에서 20% 정도가 크다고 가정했음.
  - 이런 큰 파일은 수 GB 이상인 객체도 존재함
- 버킷에 직접 업로드 시 시간이 오래 걸림
- 업로드 중간에 네트워크 문제가 생기면 처음부터 다시 업로드해야하는 문제가 존재함
  - 큰 객체는 작게 쪼갠 후 독립적으로 업로드하는 방법이 있음
  - 모든 조각이 업로드 된 후에는 그 조각을 모아서 원본 객체를 복원함
  - 이 과정을 멀티 파트(multipart) 업로드라고 함
- 멀티파트 업로드의 동작
  - 클라이언트가 멀티파트 업로드를 시작하기 위해 객체 저장소 호출
  - 데이터 저장소가 uploadID 반환. 해당 업로드를 유일하게 식별할 ID임
  - 클라이언트는 파일을 작게 분할해 업로드함.
  - 조각 하나가 업로드 될 때마다 ETag를 반환함.
    - ETag는 기본적으로 해당 조각에 대한 MD5 해시 체크섬임
    - 멀티파트 업로드가 정상적으로 되었는지 검사할 때 이용됨
  - 모든 조각을 업로드하고 나면 클라이언트는 멀티파트 업로드를 종료하라고 보냄.
    - uploadID, 조각 번호 목록, ETag 목록이 포함되어야함
  - 데이터 저장소는 전송 받은 조각 번호 목록을 사용해 원본 객체를 복원함
    - 객체 크기가 커서 복원에도 몇 분 가량 소요될 수 있음. 복원이 끝난 후에는 클라이언트에게 성공 메시지가 반환됨

### 쓰레기 수집
- 더 이상 사용되지 않는 데이터에 할당된 저장 공간을 자동으로 회수하는 절차임.
- 쓰레기 데이터가 생기는 경우
  - 객체의 지연된 삭제(lazy object deletion)
    - 삭제했다고 표시했지만 실제로 지우지 않은 경우
  - 갈 곳 없는 데이터(orphaned data)
    - 반쯤 업로드된 데이터 또는 취소된 멀티파트 업로드 데이터
  - 훼손된 데이터(corrupted data)
    - 체크섬이 일치하지 않는 데이터
- garbage collection은 주기적으로 실행됨
- 사본에 할당된 저장 공간을 회수하는 역할도 담당함
- 데이터를 다중화하는 경우 객체는 주 저장소 노드 뿐 아니라 부 저장소 노드에서도 지워야 함
- 쓰레기 수집 메커니즘
  - garbage collection은 객체를 전체 복사하는데, 삭제된 객체가 표시된 객체들은 건너뜀
  - 모든 객체를 복사한 뒤 object_mapping 테이블을 갱신함
    - object_id와 object_size 등의 값은 그대로지만, file_name과 start_offset 값은 새 위치를 가리키도록 변경됨
    - 데이터 일관성 보장을 위해 file_name과 start_offset에 대한 갱신 연산은 같은 트랜잭션 안에서 수행하는 것이 바람직 함

### Q) UUID로 객체 위치 찾을 때 오프셋이 필요한 이유는 무엇이고, 오프셋은 무엇인가?


---
# 10장 - 실시간 게임 순위표
## 1단계 - 문제 이해 및 설계 범위 확정
- 요구 사항 분석
  - Q) 순위표의 점수는 어떻게 계산하나요?
    - A) 사용자가 경기에서 승리하면 포인트를 얻고 이 포인트로 점수를 계산함. 경기에서 이길 때마다 1점의 포인트를 얻음
  - Q) 모든 플레이어가 순위표에 포함되는가?
    - A) yes
  - Q) 한 순위표는 얼마 동안 유효한가?
    - A) 매달 새로운 토너먼트 시작 시 순위표도 새로 만듦
  - Q) 상위 10명만 신경 써도 되는가?
    - A) 상위 10명과 특정 사용자의 순위를 순위표에 표시함. 시간이 되면 어떤 사용자보다 4순위 위/아래 사용자 반환 방법도 논의함
  - Q) 토너먼트 참가자 수는?
    - A) DAU는 500만 명, MAU는 2500만 명으로 함
  - Q) 토너먼트 기간 동안 평균 몇 경기가 진행되는가?
    - A) 각 선수는 하루 평균 10경기를 진행함
  - Q) 동점은 어떻게 처리 하는가?
    - A) 같은 순위로 처리함. 시간이 되면 동점 사이 우선 순위를 가르는 방법을 얘기해봄
  - Q) 순위표는 실시간인가?
    - A) 실시간 혹은 최대한 실시간에 가깝게 결과를 표시함. 누적된 결과 이력은 바람직하지 않음

### 기능 요구사항
- 순위표 상위 10명 표시
- 특정 사용자 표시
- 어떤 사용자의 4순위 위/아래 표시(보너스)

### 비기능 요구사항
- 점수 업데이트는 실시간으로 순위표에 반영
- 일반적인 확장성, 가용성 및 안정성 요구사항

### 개략적 규모 추정
- 약 초당 50명의 사용자가 게임을 플레이함
  - 사용자가 24시간 고르게 분포할 경우 DAU가 500만명일 때 초당 약 50명의 사용자가 게임을 플레이함
- 최대 부하는 평균의 5배라고 가정
  - 사용자가 고르게 분포하기 보다는 특정 시간대에 몰리기 때문임
- 따라서 초당 최대 250명의 사용자를 감당할 수 있어야 함
- 사용자 점수 획득 QPS
  - 2500
  - 하루 평균 10개로 가정하고, QPS는 약 500(50 * 10)임
  - 최대 QPS는 5배로 가정했으므로 2500임
- 상위 10명 순위표 가져오기 QPS
  - 약 50
  - 각 사용자가 하루 한 번 게임을 열고 상위 10명 순위표를 처음 게임 열 때만 표시한다고 가정함

## 2단계 - 개략적 설계안 제시 및 동의 구하기
### API 설계 - POST /v1/scores
- 사용자가 게임에서 승리하면 순위표에서 사용자 순위를 갱신
- 게임 서버에서만 호출할 수 있는 내부 API임
- 사용자는 이 API를 통하지 않고 직접 점수 업데이트할 수 없다
- 매개변수
  - user_id : 게임에서 승리한 사용자
  - points : 사용자가 얻은 포인트
- 응답
  - 200 OK : 점수 업데이트 성공
  - 400 Bad Request : 인자가 잘못 전달되어 점수 갱신이 안된 경우

### API 설계 - GET /v1/scores
- 상위 10명의 플레이어 목록
- 응답 예제
```json
{
  "data": [
    {
      "user_id": "user1",
      "user_name": "alice",
      "rank": 1,
      "score": 1000
    },
    {
      "user_id": "user2",
      "user_name": "bob",
      "rank": 2,
      "score": 977
    },
    ...
  ]
}

```

### API 설계 - GET /v1/scores/{user_id}
- 특정 사용자의 순위
- 응답 예제
```json
{
  "user_info": {
    "user_id": "user5",
    "user_name": "mike",
    "rank": 17,
    "score": 870
  }
}
```

### 개략적 설계안
- 두 가지 서비스가 포함
  - 게임 서비스
    - 게임을 플레이하는 데 사용되는 서비스
  - 순위표 서비스
    - 순위표를 생성하고 표시하는 서비스
- 매커니즘
  - 사용자가 게임을 승리하면 클라이언트는 게임 서비스에 요청을 보냄
  - 게임 서비스는 해당 승리의 정당성과 유효성을 확인 후 순위표 서비스에 점수 갱신을 요청
  - 순위표 서비스는 순위표 저장소에 기록된 해당 사용자의 점수를 갱신
  - 해당 사용자의 클라리언트는 순위표 서비스에 직접 요청해 상위 10명과 해당 사용자의 순위를 가져옴

### 클라이언트가 순위표 서비스와 직접 통신해야 하는가?
- 클라이언트가 점수를 정하는 것은 사용자가 프록시를 설치하고 점수를 마음대로 바꾸는 중간자 공격을 할 수 있기에 보안상 위험함
- 온라인 포커처럼 서버가 게임 전반을 통솔할 경우에는 게임 서버를 명시적으로 호출할 필요가 없을 수도 있음

### 게임 서비스와 순위표 서버 사이에 메시지 큐가 필요한가?
- 게임 점수가 어떻게 사용되는지에 따라 달라짐
- 다른 곳에서도 이용되거나 여러 기능을 지원해야 할 경우 카프카에 데이터를 넣는 것이 합리적일 수 있음
  - 순위표 서비스, 분석 서비스, 푸시 알림 서비스 등 여러 소비자가 다양한 데이터를 사용할 경우 메시지 큐를 사용하는 것이 좋음
  - 다른 플레이어에게 점수가 바뀌었음을 알려야 하는 순번제(턴제) 게임이나 멀티플레이 게임의 경우 더욱 그럼
- 일단 본 설계안에서는 요청 사항이 없으므로 포함하지 않음

### 데이터 모델 - RDB
- 규모 확장성이 그다지 중요하지 않고 사용자 수가 많지 않은 경우
- 각 월별 순위표는 사용자 ID와 점수 열을 갖는 DB 테이블로 표현할 수 있음
- 승리자가 신규 유저일 경우 1점을 주고, 기존 사용자인 경우 원래 점수에 1점을 추가함
- 순위의 경우 테이블을 점수 기준으로 내림차순 정렬하면 됨
  - 순위는 매번 계산할 필요가 없고, 점수만 갱신하면 됨
- 점수를 딴 경우
  - 새로운 사용자인 경우
    - INSERT INTO leaderboard (user_id, score) VALUES ({user_id}, 1)
  - 이미 존재하는 사용자인 경우
    - UPDATE leaderboard SET score = score + 1 WHERE user_id = {user_id}
- 특정 사용자 순위 검색
  - SELECT (@rownum := @rownum + 1) as rank, user_id, score FROM leaderboard ORDER BY score DESC
- 데이터가 적을 땐 좋지만, 레코드가 많아지면 성능이 매우 떨어짐
- 순위 파악을 위해서는 모든 플레이어가 순위표의 정확한 위치에 정렬되어야 함
- 같은 점수를 받는 사용자가 여럿일 수 있으므로 순위는 단순히 해당 목록 내의 사용자의 위치라고 할 수 없음
- SQL DB는 지속적으로 변화하는 대량의 정보를 신속하게 처리하지 못 함
  - 수백만 개의 레코드에 순위를 매기려면 대략 수십 초가 걸리므로 실시간성을 요구하는 경우 적합하지 않음
- 데이터가 지속적으로 변경되기에 캐시 도입도 불가능함
- RDB는 본 시스템에 요구되는 다량의 [읽기 부하](#쓰기-작업아닌가..)를 처리하기 어려움
- 일괄 작업으로 수행하면 RDB로 가능할 수도 있지만, 실시간 요구사항에 부적합함
- index를 추가하고 LIMIT를 사용하는 할 수도 있음
  - SELECT (@rownum := @rownum + 1) as rank, user_id, score FROM leaderboard ORDER BY score DESC LIMIT 10
  - 규모 확장성이 좋지 않음
  - 전체 테이블을 스캔해야하므로 성능 하락
  - 순위표 상단에 있지 않은 사용자를 찾기 어려움

### 레디스
- 메모리 기반 key-value 저장소
- 빠른 읽기 쓰기가 가능함
- 이상적인 정렬 집합(sorted set) 자료구조를 제공함
- 순위표 구현에 필요한 레디스 연산
  - ZADD
    - 기존에 없던 사용자를 집합에 삽입, 기존 사용자는 점수 업데이트.
    - 실행 시간은 O(log n)
  - ZINCRBY
    - 사용자 점수를 지정된 값만큼 증가(신규 유저는 0부터 시작한다고 가정)
    - 실행 시간은 O(log n)
  - ZRANGE/ZREVERANGE
    - 점수에 따라 정렬된 사용자 중 특정 범위의 사용자를 조회
    - 순서, 항목 수, 시작 위치를 지정 가능
    - 실행 시간은 O(log n + m)
    - m은 가져올 항목 수, n은 정렬 집합의 크기
  - ZRANK.ZREVERANK
    - 오름차순/내림차순 정렬 시 특정 사용자의 위치
    - 실행 시간은 O(log n)
- 정렬 집합을 사용한 구현의 동작 원리
  - 사용자가 점수를 획득한 경우
    - 매월 새로운 정렬 집합을 만들고 이전 순위표는 이력 데이터 저장소로 보냄
    - 게임 승리 시 ZINCRBY를 호출해 점수를 증가시키거나 사용자를 순위표 집합에 추가함
      - ZINCRBY <키> <증분> <사용자>
  - 사용자가 순위표 상위 10명을 조회하는 경우
    - 가장 높은 점수를 받은 사용자부터 내림차순 정렬한 결과를 가져옴
    - 사용자 목록과 각 사용자의 점수도 가져와야하므로 WITHSCORES 속성도 전달
      - ZRANGE leaderboard_feb_2021 0 9 WITHSCORES
  - 사용자가 자기 순위를 조회하는 경우
    - ZREVRANK leaderboard_feb_2021 <user_id>
  - 특정 사용자 순위를 기준으로 일정 범위 내 사용자를 질의하는 경우
    - ZREVERANGE leaderboard_feb_2021 {앞범위} {뒷범위}

### 정렬 집합이란?
- 집합과 유사한 자료형임
- 각 원소는 점수에 연결되어 있음
- 집합 내 원소는 고유해야하지만 같은 점수는 있을 수도 있다
- 점수는 정렬 집합 내 오름차순 정렬에 사용됨
- 내부적으로 해시 테이블과 스킵 리스트라는 두가지 자료 구조를 사용함
  - 해시 테이블은 사용자 점수를 저장하기 위해, 스킵 리스트는 특정 점수를 딴 사용자들의 목록을 저장하기 위해
  - 스킵 리스트는 빠른 검색을 가능하게 하는 자료 구조임
    - 정렬된 연결 리스트에서 다단계 색인을 두는 구조.
      - 삽입, 삭제, 검색 연산을 실행하는 시간은 O(n)임
      - 이를 빠르게 하기 위해서는 이진 탐색처럼 하는 방법이 있음
      - 중간 노드를 하나씩 건너뛰는 1차 색인과 1차 색인을 건너뛰는 2차 색인을 추가함
      - 새로운 색인이 추가될 때마다 이전 차수의 노드를 하나씩 건너 뛸 수 있음
      - 노드 사이 거리가 n-1이 되면 색인을 더 이상 추가하지 않음
      - n은 노드의 총 개수
    - 정렬 집합은 삽입이나 갱신 연산 시 모든 원소가 올바른 위치에 자동으로 배치됨
    - 원소 추가나 검색이 O(log n)임
    - 이는 RDB보다 성능이 좋음. RDB에서는 특정 사용자 순위를 계산하려면 중첩 질의문을 실행해야 함
      - SELECT *, (SELECT COUNT(*) FROM leaderboard lb2 WHERE lb2.score >= lb1.score) as rank FROM leaderboard lb1 WHERE lb1.user_id = {user_id}

### 저장소 요구사항
- 최소한 사용자 ID와 점수는 저장해야함
- 최악은 DAU 모든 사용자가 한 번 이상 승리해서 모두 순위에 올라가는 경우임
  - ID는 24자 문자열, 점수가 16비트 정수일 경우 한 항목당 26바이트가 필요함
  - 26바이트 * 2500만 = 6억 5000만 바이트 약 650MB의 저장 공간이 레디스에 필요함
- CPU 및 I/O 사용량도 고려해야함.
  - 최대 QPS가 2500이므로 단일 레디스로 충분히 감당 가능함
- 영속성은 문제가 될 수 있음
  - 레디스는 노드 장애가 발생할 수 있기 때문임
  - 레디스는 데이터를 디스크에 영속적으로 저장하는 옵션을 지원함.
  - 다만, 디스크에서 데이터를 읽어 대규모 레디스 인스턴스 재시작에는 시간이 매우 오래 걸림
  - 그래서 보통 레디스에 읽기 사본을 두는 식으로 구성함
    - 주 서버에 장애 발생 시 읽기 사본을 승격시켜서 사용함


## 3단계 - 상세 설계
### 클라우드 서비스 사용 여부 - 자체 서비스 사용
- 매월 정렬 집합을 생성해 해당 기간의 순위표를 저장함
- 해당 집합에는 사용자와 점수 정보를 저장
- 이름, 프로필과 같은 정보는 RDB에 저장함
- 순위표를 가져올 때 API 서버는 순위 데이터와 함께 RDB의 프로필 정보도 가져옴
- 장기적으로 비효율 적이지만, 상위 10명의 세부 정보는 캐시를 두어 해결할 수 있음

### 클라우드 서비스 사용 여부 - 클라우드 서비스 사용
- 기존 인프라가 AWS에 있어서 클라우드로 순위표를 구축하는 것이 자연스러운 경우
- API Gateway와 Lambda를 사용함
- API Gateway를 사용하면 RESTful API의 HTTP 엔드포인트를 정의하고 아무 백엔드 서비스에나 연결 가능함
- 람다 함수
  - API : GET /v1/scores
    - 람다 함수 : LeaderboardFetchTop10
  - API : GET /v1/scores/{user_id}
    - 람다 함수 : LeaderboardFetchPlayerRank
  - API : POST /v1/scores
    - 람다 함수 : LeaderboardUpdateScore
  - 람다는 필요할 때만 실행되며 트래픽에 따라 자동적으로 확장됨
- 게임은 API 게이트웨이를 호출하고, 게이트웨이는 적절한 람다 함수를 호출함
  - 해당 람다 함수는 스토리지 계층(레디스나 RDB)의 명령을 호출해 얻은 결과를 API 게이트웨이에 반환하고, API 게이트웨이는 그 결과를 애플리케이션에 전달함
  - 서버 인스턴스 없이 질의를 실행할 수 있음

### 레디스 규모 확장
- 500만 DAU는 한 대로도 지원 가능함
- 5억 DAU가 될 경우 최악에는 65GB(650MB * 100) 정도의 저장 공간이 필요하고, 250_000(2500 * 100) QPS를 처리해야함
  - 이 정도 규모를 감당하기 위해서는 샤딩이 필요함
  
### 레디스 규모 확장 - 데이터 샤딩 방안 : 고정 파티션
- 순위표에 등장하는 점수 범위에 따라 파티션을 나누는 방안임
- 한 달 동안 얻을 수 있는 점수 범위가 1 ~ 1000일 때 그 데이터를 범위 별로 나누는 것임
  - 특정 사용자의 점수를 입력하거나 갱신 시 해당 사용자가 어느 샤드에 있는지 알아야 함
  - 사용자 점수가 높아져 다른 샤드로 옮겨야할 수 있다는 것도 주의해야 함
  - 상위 10명은 가장 높은 샤드에서 상위 10명을 가져오면 됨
  - 특정 사용자의 순위를 알기 위해서는 해당 사용자가 속한 샤드 내 순위 + 해당 샤드보다 높은 점수의 샤드의 모든 사용자도 알아야 함
  - info keyspace 명령어를 사용하면 특정 샤드에 속한 모든 사용자 수를 O(1)로 알 수 있음

### 레디스 규모 확장 - 데이터 샤딩 방안 : 해시 파티션
- 레디스 클러스터를 이용
  - 여러 노드에 데이터를 자동으로 샤딩하는 방법을 제공함
- 점수가 특정 대역에 과도하게 모여있는 경우 유용함
- 안정 해시는 사용하지 않지만, 각각 키가 특정한 해시 슬롯에 속하도록 샤딩 기법을 사용함
  - 총 해시 슬롯 수는 16384개로 고정되어 있음
    - CRC16(key) % 16384 연산으로 어떤 키가 어느 슬롯에 있는지 계산함
  - 모든 키를 재분배하지 않아도 클러스터에 쉽게 노드를 추가하거나 제거 가능
- 점수 갱신 시에는 해당 사용자의 샤드를 찾아 해당 사용자 점수만 변경하면 됨
- 상위 10명의 플레이어 검색은 쉽지 않음.
  - 모든 샤드에서 상위 10명을 받아 애플리케이션 내에서 다시 정렬하는 분산-수집(scatter-gather) 접근법을 사용해야 함
    - 상위 k개의 결과를 반환해야 할 때 k가 클 수록 각 샤드의 많은 데이터를 읽어 정렬해야 하므로 지연 시간 증가
    - 가장 느린 파티션의 데이터를 다 읽어야 계산이 가능함
    - 특정 사용자의 순위를 결정할 간단한 방법이 없음
- 따라서 여기서는 고정 파티션을 사용

### 레디스 노드 크기 조정
- 쓰기 작업이 많은 애플리케이션에는 많은 메모리가 필요함
  - 장애에 대비해 스냅샷을 생성할 때마다 모든 쓰기 연산을 감당할 수 있어야 함
- 쓰기 연산이 많은 애플리케이션을 메모리를 2배 더 할당하는 것이 안전함
- 레디스는 성능 벤치마킹을 위해 redis-benchmark라는 도구를 제공함
  - 여러 클라이언트가 동시에 여러 질의를 실행하는 것을 시물레이션해 초당 얼마나 많은 요청을 처리할 수 있는지 측정함

### 대안 : NoSQL
- 대안이 될 수 있는 NoSQL
  - 쓰기 연산에 최적화되어 있어야 함
  - 같은 파티션 내의 항목을 점수에 따라 효율적으로 정렬 가능해야 함
  - DynamoDB, 카산드라, MongoDB 등이 있음
- DynamoDB를 사용함
  - 기본 키 외의 속성으로 효과적으로 질의할 수 있도록 전역 보조 색인을 제공함
  - 부모 테이블의 속성들로 구성되지만, 기본키는 부모 테이블과 다름
  - 사용자 정보와 순위표 등 순위표 화면 표시에 필요한 모든 정보를 담고 있음.
    - 규모 확장성이 좋지 않은 단점이 있음
    - 레코드가 많아지면 상위 점수를 찾기 위해 전체 테이블을 뒤져야 함
    - game_name#{year-month}을 파티션 키로, 점수를 정렬 키로 사용하면 테이블 전체를 읽는 일을 피할 수 있음
  - 부하가 높을 경우 문제가 됨.
    - 안정 해시를 이용해 여러 노드에 데이터를 분산함
    - 여러 파티션에 고르게 분산되어야 하지만, 한 달치 데이터가 동일한 파티션에 저장되므로 핫 파티션이 되어 버림
  - 데이터를 n개의 파티션으로 분할하고 파티션 번호를 파티션 키에 추가하는 것
    - 쓰기 샤딩이라고 부름
    - 읽기와 쓰기 작업 모두를 복잡하게 하므로 잘 따져봐야 함
- 특정한 달의 데이터를 읽을 땐 모든 파티션의 질의 결과를 합쳐야 하므로 구현이 복잡함
- 전역 보조 색인은 game_name#{year-month}#p{partition_number}을 파티션 키로, 점수를 정렬 키로 사용함
  - 같은 파티션 내 데이터는 전부 점수 기준으로 정렬된 n개의 파티션이 만들어 짐
  - 상위 10명은 분산-수집 접근법을 사용함
- 파티션 수를 결정할 땐 신중해야 함.
  - 파티션 수가 많으면 각 파티션의 부하는 줄지만, 최종 순위표를 만들 땐 읽어야 할 파티션이 많아서 복잡해짐
- 사용자 위치를 정확하게 표시하는 것이 아니라, 상위 10~20%와 같이 표기하는 것도 좋을 수 있음
  - 모든 샤드의 점수 분포가 거의 같다고 가정하면 각 샤드의 점수 분포 분석 결과를 캐시하는 크론 작업(cron job)을 만들 수 있다.
  - 그렇게하면 백분위를 빠르게 구할 수 있다

# 4단계 - 마무리
### 더 빠른 조회 및 동점자 순위 판정 방안
- 레디스 해시를 사용하면 문자열 필드와 값 사이 대응관계를 저장해둘 수 있음
  - 순위표에 표시할 사용자 ID와 사용자 객체 사이의 대응관계를 저장.
    - DB에 질의하지 않아도 사용자 정보를 빠르게 확인 가능
  - 두 사용자의 점수가 같은 경우 누가 먼저 점수를 받았는지 순위를 매길 수 있음
    - 사용자 ID와 해당 사용자가 마지막으로 승리한 경기의 타임 스탬프 사이 대응관계를 저장
    - 동점자가 발생하면 타임스탬프 값을 기준으로 가능함

### 시스템 장애 복구
- 사용자가 이길 때마다 RDB에 타임스탬프와 함께 이를 활용하는 스크립트를 만들면 간단히 복구 가능함
- 사용자별로 모든 레코드를 훑으면서, 레코드 당 한 번씩 ZINCRBY를 호출
- 대규모 장애가 발생해도 오프라인 상태에서 순위표를 복구할 수 있음.

### 쓰기 작업아닌가..?