# 1장 - 근접성 서비스
- 근접성 서비스는 현재 위치에서 가까운 시설을 찾는 데 이용

## 1단계 - 문제 이해 및 설계 범위 확정
- 설계 범위 줄이기
  - Q) 사용자가 검색 범위를 지정할 수 있어야 하는가? 반경 내 표시할 사업장이 충분하지 않을 경우 시스템이 알아서 확장해도 되는가?
    - A) 일단 주어진 반경 내의 사업장만 대상으로 함. 시간이 남으면 이후 처리를 고려
  - Q) 최대 반경은? 20km로 해도 되는가?
    - A) ok
  - Q) 사용자가 UI에서 검색 반경을 변경할 수 있는가?
    - A) 네. 0.5km, 1km, 2km, 5km, 20km 중 선택 가능
  - Q) 사업장 정보는 어떻게 시스템에 추가/삭제/갱신 되는가? 사업장에 대한 작업 정보가 실시간으로 유저에게 보여야 하는가?
    - A) 소유주가 정보를 추가/삭제/갱신 할 수 있어야 함. 새로 추가되거나 갱신한 정보는 다음 날까지 반영됨
  - Q) 사용자가 이동 중 앱이나 웹사이트 이용 시 항상 현재 위치 기준을 유지하기 위해 자동 갱신해야 하는가?
    - A) 사용자 이동 속도가 그리 빠르지 않아서 상시 갱신은 필요 없다고 가정
- 기능 요구사항
  - 사용자의 위치(경도와 위도 쌍)와 검색 반경 정보에 매치되는 사업장 목록 반환
  - 소유주가 사업장 정보를 추가/삭제/갱신할 수 있도록 하되, 그 정보가 실시간 반영은 아니어도 된다
  - 고객은 사업장의 상세 정보를 볼 수 있어야 함
- 비기능 요구 사항
  - 낮은 응답 지연
  - 데이터 보호
    - 위치 기반 서비스(LBS, Location-Based Service)는 사용자의 위치 정보를 다루므로 데이터 보호가 중요
    - GDPR(General Data Protection Regulation)
    - CCPA(California Consumer Privacy Act)
  - 고가용성, 규모 확장성

### 개략적 규모 추정
- DAU 1억명
- 등록된 사업장 수 2억
- QPS : 5000
  - 하루 평균 5회 검색한다고 가정
  - 하루를 10만초로 가정
  - 1억 * 5 / 10만 = 5000

## 2단계 - 개략적 설계안 제시 및 동의 구하기
### API 설계
- GET /v1/search/nearby
  - 특정 검색 기준에 맞는 사업장 목록을 반환
  - 보통 페이지 단위로 나눠 반환
  - 요청
    - `latitude` : 사용자의 위도
    - `longitude` : 사용자의 경도
    - `radius` : 검색 반경
    - 응답
      ```json 
        {
          "total": 10,
          "businesses": [{business object}]
        }
      ```
      - business object에는 사업장 상세 정보들이 담긴다

### 데이터 모델 - 읽기/쓰기 비율 
- 읽기 연산이 매우 자주 수행됨.
  - 주변 사업장 검색
  - 사업장 정보 확인
- 쓰기 연산의 빈도는 적음.
- 읽기 연산이 압도적일 결우 RDB가 유리할 수 있다.
      
### 개략적 설계안
- 로드밸런서
  - 유입 트래픽을 자동으로 여러 서비스에 분산
  - 로드밸런서를 단일 DNS 진입점(entry point)로 지정하고, URL 경로를 분석해 어느 서비스에 트래픽을 전달할지 결정
- 위치 기반 서비스(LBS)
  - LBS는 시스템의 핵심 부분
  - 주어진 위치와 반경 정보를 이용해 주변 사업장을 검색
  - 특징
    - 쓰기 요청이 없는, 읽기 요청만 빈번하게 발생
    - QPS가 높음.
      - 특정 시간대의 인구 밀집 지역일수록 심함
    - 무상태 서비스라서 수평적 규모 확장이 쉬움
- 사업장 서비스
  - 사업장 소유주가 사업장 정보를 생성, 갱신, 삭제함.
  - 기본적으로 쓰기 요청으로 QPS가 낮음
  - 고객이 사업장 정보를 조회.
    - 특정 시간대에 QPS가 높아짐
- DB 클러스터
  - primary-secondary DB 형태로 구성할 수 있음.
  - 주 DB는 쓰기 요청을 처리, 부 DB는 읽기 요청을 처리
  - 복제에 걸리는 시간(delay) 때문에 주 DB와 부 DB의 데이터에 차이가 있을 수 있음.
    - 사업장 정보가 실시간으로 갱신될 필요가 없어서 보통 문제되지 않음.
- 사업장 서비스와 LBS의 규모 확장성
  - 둘 다 무상태 서비스이므로 확장과 축소가 쉬움

### 주변 사업장 검색 알고리즘
- Redis의 지오해시(Geohash)나 PostGIS 확장을 설치한 Postgres DB를 활용함
- 내부 구조는 어려우니 몰라도 될 수 있음.
- DB의 이름 나열보다는 지리적 위치 색인이 어떻게 동작하는지 설명해 문제 풀이 능력과 기술 지식을 강초하는 것이 좋음.

### 주변 사업장 검색 알고리즘 방안 1 - 2차원 검색
- 주어진 반경으로 그린 원 안에 놓인 사업장을 검색하는 방법
- 가장 직관적이지만, 지나치게 단순하다는 문제가 있음
- SQL 문
  - SELECT * FROM businesses WHERE (latitude BETWEEN {:my_lat} - radius AND {:my_lat} + radius) AND (longitude BETWEEN {:my_long} - radius AND {:my_long} + radius)
  - 전체 테이블을 읽으므로 비효율적
- 위도 경도 칼럼에 색인 사용
  - 위도와 경도 각각에 해당하는 데이터 집합은 금방 추출 가능
  - 두 집합의 교집합을 구할 때 효율적이지 않음.
- **DB 색인은 한 차원의 검색 속도만 개선**
- 지리적 정보를 색인하는 방법
  - 해시 기반
    - 균등 격자(even grid), 지오해시(geohash), 카르테시안 계층(cartiesian tiers) 등
  - 트리 기반
    - 쿼드트리(quadtrees), 구글 S2, R-트리(R-trees) 등
  - 각 구현 방법은 서로 다르지만, 개략적 아이디어는 동일함
    - **지도를 작은 영역으로 분할하고 고속 검색이 가능하도록 색인을 만듦**
    - 지오해시, 쿼드트리, 구글 S2가 실제로 많이 사용됨

### 주변 사업장 검색 알고리즘 방안 2 - 균등 격자
- 지도를 격자 또는 구획으로 나누는 단순한 접근법
- 사업장의 분포가 균등하지 않다는 문제가 있음
- 이상적으로는 인구 밀집 지역에는 작은 격자를, 드문 지역에는 큰 격자를 사용하면 좋음
- 인접 격자를 찾기 까다로울 수 있음

### 주변 사업장 검색 알고리즘 방안 3 - 지오해시(Geohash)
- 2차원의 위도 경도 데이터를 1차원의 문자열로 변환
- 비트를 하나씩 늘려 재귀적으로 세계를 더 작은 격자로 분할해 나감
- 자오선과 적도선을 기준으로 사분면을 나눔
  - 그 각각의 격자를 다시 사분면으로 나눔
  - 이런 식으로 계속 나눠나가면서 격자를 만듦
  - 이 절차를 원하는 정밀도까지 반복
  - 통상적으로 base32 표현법을 사용
- 지오해시는 12단계 정밀도를 가짐
- 보통 4 ~ 6단계 사이를 사용
- 최적 정밀도
  - 사용자가 지정한 반경으로 그린 원을 덮는 최소 크기 격자를 만드는 지오해시 길이를 구함
  - 격자 가장자리 처리 방식에 대한 경계 조건이 있음
- **격자 가장자리 관련 이슈**
  - 해시값의 공통 접두어(prefix)가 긴 격자들이 서로 더 가깝게 놓이게 보장함
  - 이슈 1
    - 하지만, 가깝다고 같은 접두어는 아닐 수 있음.
    - 적도의 다른 쪽에 놓이거나 자오선상으로 다른 곳에 위치할 경우임.
    - 단순한 SQL 문으로는 모든 사업장을 못 가져옴
    - SELECT * FROM geohash_index WHERE geohash LIKE 'prefix%'
  - 이슈 2
    - 공통 접두어는 유사하지만 서로 다른 격자에 놓이는 경우
    - 가장 간단하게는 현재 격자를 비롯한 모든 인근 격자의 정보를 가져옴.
- 표시할 사업장이 충분하지 않을 경우
  - 선택지 1 : 주어진 반경 내 사업장만 반환
    - 사용자가 만족하지 못할 수 있음
  - 선택지 2 : 검색 반경을 키움
    - 지오해시 값의 마지막 비트를 삭제해 검색 반경을 늘림
    - 충분한 사업장이 확보될 때까지 확대함


### 주변 사업장 검색 알고리즘 방안 4 - 쿼드트리
- 격자의 내용이 특정 기준을 만족할 때까지 2차원 공간을 재귀적으로 사분면 분할하는 데 흔히 사용되는 자료 구조
- 쿼드트리는 질의에 사용될 트리 구조를 메모리 안에 만드는 것임
- 해당 자료 구조는 각각의 LBS 서버에 존재해야 하고, 서버가 시작하는 시점에 구축됨
```java
public void buildQuadtree(TreeNode node) {
    if (countNumberOfBusinessesInCurrentGrid(node) > 100){
        node.subdivide();
        for (TreeNode child : node.getChildren()) {
            buildQuadtree(child);
        }
    }
}
```
- 쿼드트리를 전부 저장하는 데 사용되는 메모리
  - 123
  - 말단 노드
    - 격자 식별에 사용되는 좌상단과 우하단 꼭짓점 좌표 : 32바이트
    - 격자 내부 사업장 ID 목록 : ID당 8바이트 * 100
    - 총 832바이트
  - 내부 노드
    - 격자 식별에 사용되는 좌상단과 우하단 꼭짓점 좌표 : 32바이트
    - 하위 노드 4개를 가리킬 포인터 : 32바이트
    - 총 64바이트
  - 격자 안 최대 100개의 사업장이라고 가정
  - 말단 노드
    - 약 200m / 100 = 약 2m(200만)
  - 내부 노드
    - 2m * 1/3 = 약 0.67m(67만)
  - 총 메모리 요구량
    - (200만 * 832바이트) + (67만 * 64바이트) = 1.66GB + 0.43GB = 약 1.71GB
  - 1.7기가의 쿼드트리 메모리 요구량을 감안하면 충분히 서버에 올릴 수 있음
- 쿼드트리 구축 소요 시간
  - 시간 복잡도는 nlogn 이므로 몇 분 정도의 시간이 소요됨.
- 쿼드트리로 주변 사업장 검색
  - 쿼드트리 인덱스가 메모리에 구축됨
  - 검색 시작점이 포함된 말단 노드를 만날 때까지 탐색 진행
  - 해당 노드에 100개의 사업장이 있으면 해당 노드만 반환
  - 그렇지 못할 경우에만 충분한 사업장 수가 확보될 때까지 인접 노드 추가
- 쿼드트리 운영 시 고려 사항
  - 200m 개의 사업장에 대한 쿼드트리 구축 시 서버 시작 시간이 길어질 수 있음.
    - 무중단 배포가 필요해질 수 있음.
  - 사업장 추가/제거 시 쿼드트리 갱신 문제
    - 한 번에 모든 서버를 갱신하면 DB에 부하가 심해질 수 있음
    - 점진적인 갱신으로 해결 가능
    - 수많은 key가 한 번에 무효화되어 캐시 서버에 부하가 가해질 수 있음
    - 실시간 갱신 -> 락 등을 사용해야해서 설계가 복잡해짐

### 주변 사업장 검색 알고리즘 방안 5 - 구글 S2
- 구글 S2 기하(geometry) 라이브러리는 많이 사용되는 라이브러리임
- 메모리 기반(in memory)
  - 지구를 힐베르트 곡선이라는 공간 채움 곡선을 이용해 1차원 색인화하는 방안
  - **힐베르트 곡선에서는 인접한 두 지점은 색인화 후 1차원 공간에서도 인접한 위치를 보장**
- 임의 지역에 다양한 수준의 영역 지정이 가능
- 전 세계 지리적 영역에 설정한 가상의 경계
- 영역 지정도 가능함.
  - 고정된 정밀도를 사용하는 대신 최소 수준, 최고 수준, 최대 셀 개수 등 지정 가능

### 지오해시 vs 쿼드트리
- 지오해시
  - 구현과 사용이 쉬움(트리 구축이 없음)
  - 지정 반경 이내 사업장 검색을 지원
  - 정밀도 고정 시 격자 크기도 고정
  - 동적으로 격자 크기 조절은 어려움
  - 색인 갱신이 쉬움
    - 삭제 시 단순하게 열 하나만 삭제하면 됨
- 쿼드트리
  - 구현이 더 까다로움(트리 구축 필요)
  - k번째로 가까운 사업장까지의 목록을 구할 수 있음
    - 사용자는 반경에 상관 없이 가까운 사업장 k개를 찾기 원할 수도 있음
    - k개의 사업장을 찾을 때까지 검색 범위를 자동으로 조절할 수 있음
  - 인구 밀도에 따라 격자 크기를 동적으로 조정할 수 있음
  - 지오해시보다 색인 갱신이 까다로움
    - 색인 삭제 시 루트 노드부터 말단 노드까지 순회해야 함
    - 색인 갱신의 시간 복잡도는 logn
  - 다중 스레드 사용 시 락을 사용해야할 수 있음
  - 트리 균형을 다시 맞추는 리밸런싱(rebalancing)이 필요하면 더 복잡해짐

  
## 3단계 - 상세 설계
### DB 규모 확장
- 사업장 테이블
  - 사업장 정보는 한 서버에 담을 수 없을 수 있음.
  - 샤딩하기에 적합한 테이블임.
  - 샤딩하는 가장 간단한 방법은 사업장 ID를 기준으로 하는 것임.
    - 모든 샤드에 부하를 고르게 분산할 수 있을 뿐 아니라 운영 측면에서도 관리하기 쉬움
- 지리 정보 색인 테이블
  - 지오해시 테이블 구성 방법 1
    - 각 지오해시에 연결되는 모든 사업장 ID를 JSON 배열로 같은 열에 저장하는 방법
    - 특정 지오해시에 속하는 모든 사업장 ID가 한 열에 보관
  - 지오해시 테이블 구성 방법 2
    - 같은 지오해시에 속한 사업장 ID 각각을 별도 열로 저장
      - 사업장 하나에 한 레코드
  - 방법 1 JSON 배열로 저장 시 정보 갱신에서 불리함
    - JSON 배열에서 중복되는지 여부 검사도 해야하고, 병렬 실행에서 갱신 연산으로 인한 데이터 손실을 막기 위해 락을 사용해야함.
  - 색인 전부를 DB 한 대에서 모두 보관 가능함
  - 지오해시는 샤딩하기 쉽지 않으므로 섣부르게 샤딩하지 않는 게 좋을 수 있음

### 캐시
- 캐시 도입 전 정말 필요한지에 대한 의문부터 해결해야 함
- 처리 부하가 읽기 중심이고, DB 크기가 작아서 하나의 DB로 수용 가능함.
- 이런 경우 질의문 처리 성능은 I/O에 좌우되지 않으므로 메모리 캐시를 사용할 때와 비슷함
- 읽기 성능이 병목이라면 사본 DB를 증설해서 읽기 대역폭을 늘릴 수 있음
- 캐시 키
  - 사용자 위치의 위도 경보 정보
    - 사용자 전화기에서 반환되는 위치 정보는 추정치일 뿐 아주 정확하지 않고, 매 측정마다 조금씩 달라짐
    - 사용자가 이동하면 위도와 경도 정보도 미세하게 변경됨.
    - 따라서 캐시 키로 적합하지 않음.
- 캐시 데이터 유형
  - 지오해시 - 해당 격자 내의 사업장 ID 목록
    - 사업장 정보는 자주 변경되지 않음.
    - 지오해시에 대응하는 사업장 목록을 요청받으면 캐시를 우선 조회 후 없을 경우 DB에서 가져온 다음 캐시에 저장
    - 사업장 추가/수정/삭제 시 DB 갱신하고 캐시에 보관된 항목은 무효화해야함.
```java
public List<String> getNearbyBusinessIds(String geohash) {
    String cacheKey = hash(geohash);  
    List<String> listOfBusinessIds = cache.get(geohash);
    if (listOfBusinessIds == null) {
        businessIds = db.getNearbyBusinessIds(geohash);
        cache.put(geohash, listOfBusinessIds, "1d");
    }
    return listOfBusinessIds;
}
```
  - 사업장 ID - 사업장 정보 객체

### 지역(region) 및 가용성 구역
### 시간대 또는 사업장 유형에 따른 검색

### 추가 질문 : 시간대 혹은 사업장 유형별 검색
- 지금 영업 중인 사업장 혹은 식당 정보만 받아오고 싶을 경우
  - 앞서 나온 지오해시나 쿼드트리 같은 방법을 사용후 에 검색 결과로 나온 사업장 수는 상대적으로 적음
  - 사업장 ID를 전부 확보한 이후 사업장 정보를 전부 추출해서 필터링을 사용

### 최종 아키텍처 다이어그램
- 주변 사업장 검색 - 주변 반경 500미터 이내 모든 식당 찾기
  - 클라이언트 앱은 사용자 위치와 검색 반경을 로드밸런서로 전송
  - 로드밸런서는 해당 요청을 LBS로 보냄
  - 사용자 위치와 반경을 바탕으로 LBS는 검색 요건을 만족하는 지오해시 길이를 계산
  - LBS는 인접한 지오해시를 계산한 다음 목록에 추가
  - 각 지오해시에 대한 사업장 ID 레디스 캐시를 조회
  - 사업장 ID를 사용해 사업장 정보 레디스 캐시를 조회
  - 상세 정보에 의거해 사용자와의 거리를 계산하고 우선순위를 매긴 후 클라이언트에게 반환
- 사업장 정보 조회, 갱신, 추가, 삭제
  - 모든 사업장 정보 관련 API는 LBS와는 분리되어 있음.
  - 사업장 정보가 캐시에 기록되어 있는지 살펴봄.
  - 캐시에 없을 경우에는 DB 조회 후 캐시에 저장하고 반환
  - 새로 추가하거나 갱신 정보는 비실시간일 경우에는 트래픽이 적은 시간에 작업을 진행


## 4단계 - 마무리


### 추가 해 볼 내용
- 직접 지도 데이터 이용해서 테스트해보기
- 각 방안에 대해서도 실습해보기

### 위도, 경도를 그대로 사용한 2차원 검색
- what?
  - 위도에 해당하는 수 많은 데이터가 존재
    - 인덱스를 걸 수 있음.
  - 경도에 해당하는 수 많은 데이터가 존재
    - 인덱스를 걸 수 있음.
  - 위도, 경도만 해도 엄청난 데이터가 존재함. 이 데이터의 합집합을 구하는 것은 적지 않은 과부하가 발생할 수 있음.
- why?
  - 컬럼 하나에 대한 인덱스는 구하기 쉬울 수 있음.
  - 하지만 두 컬럼에 대한 인덱스를 구하기는 어려움
  - 위도와 경도를 1차원에 표현할 수 있게 해야 함.
- how?
  - 지오해시와 쿼드트리 같은 2차원 데이터를 1차원 데이터로 만들 수 있게 하기


### 지오해시
- what?
  - 2차원 위도, 경도 데이터를 1차원으로 압축 시킬 수 있음.
    - 문자열로 사용할 수 있다.
  - 기준
    - 자오선, 적도선
  - 12단계
    - 보통은 4~6단계를 사용
- why?
  - 위에 [위도, 경도를 그대로 사용한 2차원 검색]를 참고.
- how?
  - 비트를 늘려서 "재귀적"으로 사분면을 점점 쪼개나감.
  - 원하는 정도의 정밀도까지 쪼개나감.
  - 2진 값을 만든 후 base32로 변환한다.
    - 인코딩해 해쉬 값을 보여준다.
- problem?
  - 격자 가장자리 문제
    - 근처에 위치하는데, prefix가 다를 수 있음.
  - 해당 격자 내의 데이터가 충분하지 않는 상황이 발생할 수 있음
    - prefix를 하나씩 줄여가면서 검색 반경을 늘림
    - 그대로 표현하는 방법이 있음.

### 쿼드트리
- what?
  - 이름처럼 트리 사용
  - 4개의 사분면으로 분할하기에 **쿼드**트리라고 부른다.
  - 디스크에 저장하는 방식이 아니라, 메모리에 트리를 저장하는 방식을 사용한다.
- why?
  - 위에 [위도, 경도를 그대로 사용한 2차원 검색]를 참고.
- how?
  - 트리 말단 노드는 지정된 사업장 수에 해당하는 사업장 목록을 갖고 있음
  - ~~각 노드~~ 리프 노드는 특정 기준을 만족할 수 있다.
    - 사업장 개수 100개 등
  - 각 사분면을 점진적으로 쪼개나가면서 사업장 갯수 기준을 만족할 때까지 말단노드로 나아감
  - 디스크가 아니라 서버가 실행될 때 메모리에 적재하는 방식이다.
  - 사업장 검색
    - 검색 목표가 포함된 말단 노드를 만날 때까지 탐색
    - 검색 목표를 만족할 때까지 검색 범위를 자동 조정할 수 있다.
- problem?
  - 유저가 찾고자하는 최대 사업장의 갯수가 유동적인 데이터일 경우 쿼드트리를 새로 만들거나 쿼드트리 하나만으로 처리하기 힘들 수 있다.
  - 멀티 스레드에서 락이 필요함
  - 트리에 데이터를 추가/제거하는 것이 어려울 수 있다.
    - 트리 리밸런싱 시 부하가 증가한다.
  - 메모리 오버헤드가 클 수 있음
  - 데이터가 불균형일 경우 오히려 성능이 저하될 수 있을 것 같음
    - 강남 같은 경우 등 너무 많은 뎁스가 생길 수 있음
  - 초기 실행 시간이 증가할 수 있다.
    - 서버들을 분산시켜서 프로비저닝할 필요가 있음.
    - 메모리가 완전히 준비된 이후 요청이 들어올 수 있게 처리가 필요함
  - 캐시를 확인하기 위해서 최소 통합 테스트까지 강제될 수 있음
    - 메모리에 트리가 적재되었는지 확인이 필요하기 때문

### Q) 쿼드트리에서 최대 사업장 갯수가 변경되면 각 max value에 해당하는 쿼드트리를 하나씩 만들어야 하는가?
- 서버 상 쿼드 트리 구축 시 default max value를 설정해두고, 이후에 변경이 필요할 때는 새로운 쿼드 트리를 구축해야 할 것 같음.
- 동적으로 max value를 설정할 수 있을 경우 적합하지 않을 수 있을 것 같음.
- 기본 쿼드 트리를 하나 만들어두고, max value에 따라 기존 쿼드 트리를 활용해서 사용한다고 함.



---
# 2장 - 주변 친구
- 1장의 근접성 서비스와의 차이는 사업장 주소는 정적이지만, 주변 친구의 위치는 동적이라는 점이다.

## 1단계 - 문제 이해 및 설계 범위 확정
- 질의응답
  - Q) 지리적으로 얼마나 가까워야 '주변에 있다'고 하는가?
    - A) 5마일, 수치는 설정 가능함
  - Q) 그 거리가 두 사용자 사이의 직선거리로 가능하는가?
    - A) 넹
  - Q) 얼마나 많은 사용자가 해당 앱을 사용하는가? 10억명에 10% 정도가 해당 기능을 사용한다고 가정해도 되는가?
    - A) 넹
  - Q) 사용자의 이동 이력을 보관해 두는가?
    - A) 넹
  - Q) 친구 관계에 있는 사용자가 10분 이상 비활성 상태면 해당 사용자가 사라지는가, 아니면 마지막 위치를 표시하는가?
    - A) 사라지게 함
  - Q) GDPR(General Data Protection Regulation)이나 CCPA(California Consumer Privacy Act)와 같은 규정을 준수하는가?
    - A) 일단은 생략함
- 기능 요구 사항
  - 사용자는 모바일 앱에서 주변 친구를 확인할 수 있어야 함.
  - 각 친구와의 거리와 마지막 갱신 시각을 함께 표시
- 비기능 요구 사항
  - 낮은 지연 시간
  - 안정성
  - 결과적 일관성
    - 위치 데이터에 강한 일관성까지는 필요 없음. 어느정도 시간이 걸리는 것도 용인함

### 개략적 규모 추정
- 주변 친구는 5마일(8km) 반경 이내 친구로 정의
- 위치 정보는 30초 주기로 갱신
- 주변 친구 검색 기능을 활용하는 사용자는 1억명으로 가정
- 동접자수는 DAU의 10%, 1천만명으로 가정
- 평균적으로 한 사용자는 400명의 친구를 갖는다고 가정, 모든 사용자가 주변 친구 검색 기능을 활용한다고 가정
- 페이지 당 20명의 주변 친구를 표시하고, 사용자 요청이 있으면 더 많은 친구를 표시
- QPS : 약 334_000
  - DAU 1억
  - 동시 접속 1천만
  - 30초마다 자기 위치를 시스템에 전송
  - 1천만 / 30초 = 334_000

## 2단계 - 개략적 설계안 제시 및 동의 구하기
### 개략적 설계안
- P2P를 활용한 방식으로도 해결 가능함.
  - 모바일 단말은 연결 상태가 좋지 않을 수도 있고, 사용할 수 있는 전력이 충분하지 않아서 실용적인 아이디어는 아님
- 공용 백엔드를 사용하면 좀 더 실용적으로 처리 가능
  - 공용 백엔드의 역할
    - 모든 활성 상태 사용자의 위치 정보를 수신
    - 위치 변경 내역 수신 시 마다 사용자의 모든 활성 상태 친구를 찾아 해당 단말로 변경 내용을 전달
    - 두 사용자 사이 거리가 임계치보다 멀면 변경 내용 전송X
    - 큰 규모에서는 적용하지 쉽지 않음.
      - 30초 마다 갱신 -> 초당 334_000번의 위치 정보 갱신
      - 400명의 친구, 10% 활성 상태일 경우 초당 334_000 * 400 * 0.1 = 13_360_000번의 메시지 전송

### 설계안
- 우선 소규모 백엔드를 위한 설계안으로 시작
- 로드밸런서
  - RESTful API 서버 및 양방향 유상태(stateful) 서버로 트래픽을 분산
- RESTful API 서버
  - 무상태 API 서버 클러스터
  - 통상적인 요청/응답 트래픽을 처리
- 웹소켓 서버
  - 친구 위치 정보 변경을 실시간 처리하는 유상태 서버 클러스터
  - 각 클라이언트는 한 서버와 웹소켓 연결을 지속적으로 유지
- 레디스 위치 정보 캐시
  - 활성 상태 사용자의 가장 최근 위치 정보 캐시
  - 레디스 내에 TTL(Time To Live)를 사용해 비활성 상태 유저의 정보는 캐시에서 삭제
  - 레디스 외에도 TTL 지원하는 key-value 저장소는 캐시로 활용 가능
- 사용자 DB
  - 사용자 데이터와 친구 관계 정보 등을 저장
  - RDB, NoSQL 어느 쪽이든 사용 가능
- 위치 이동 이력 DB
  - 사용자 위치 변동 이력을 보관
  - 주변 친구 표시와 직접 관계된 것은 아님
- 레디스 펍/섭(Pub/Sub) 서버
  - 초경량 메시지 버스(message bus)
  - 아주 싼 값에 새로운 채널 생성 가능
  - GB급 메모리를 갖춘 최신 레디스 서버는 수백만 개의 채널을 생성 가능
  - 웹소켓 서버를 통해 수신한 사용자의 위치 정보 변경 이벤트를 배정된 펍/섭 채널에 발행
  - 해당 사용자의 친구 각각과 연결된 웹소켓 연결 핸들러는 해당 채널의 구독자로 설정됨
  - 특정 사용자 위치가 바뀌면 모든 친구의 웹소켓 연결 핸들러 호출
- 주기적 위치 갱신
  - 모바일 클라이언트는 항구적으로 유지되는 웹소켓 연결을 통해 주기적으로 위치 변경 내역을 전송
  - 모바일 클라이언트가 위치 변경 사실을 로드밸런서에 전송
  - 로드밸런서는 해당 클라와 웹소켓 서버 사이 연결을 통해 웹소켓 서버로 보냄
  - 웹소켓 서버는 해당 이벤트를 위치 이동 이력 DB에 저장
  - 웹소켓 서버는 세 위치를 위치 정보 캐시에 보관, TTL도 새롭게 갱신
  - 웹소켓 서버는 레디스 펍/섭 서버의 해당 사용자 채널에 새 위치를 발행
  - 레디스 펍/섭 채널에 발행된 이벤트는 모든 구독자에게 브로드캐스트됨.(온라인 상태 유저)
  - 메시지를 받은 웹소켓 서버는 '위치를 보낸 사용자'와 '받은 사용자' 사이의 거리를 계산
  - 사용자 검색 반경을 넘으면 갱신하지 않고, 거리 내 일 경우 갱신함

### API 설계
- 웹소켓
  - 위치 정보 변경 내역을 전송하고 수신
- 필요한 API
  - [서버] 주기적인 위치 정보 갱신
    - 요청 : 클라이언트는 위도, 경도 시각 정보를 전송
    - 응답 : 성공/실패
  - [클라이언트] 클라이언트가 갱신된 친구 위치를 수신하는 데 사용할 API
    - 전송되는 데이터 : 친구 위치 데이터와 변경된 시각을 나타내는 타임스탬프
  - [서버] 웹소켓 초기화 API
    - 요청 : 클라이언트는 위도, 경도 시각 정보를 전송
    - 응답 : 클라이언트는 자기 친구들의 위치 데이터를 수신
  - [클라이언트] 새 친구 구독 API
    - 요청 : 웹소켓 서버는 친구 ID 전송
    - 응답 : 가장 최근 위도, 경도, 시각 정보 전송
  - [클라이언트] 구독 해지 API
    - 요청 : 웹 소켓 서버는 친구 ID 전송
    - 응답 : 성공/실패
  - HTTP 요청
    - API 서버는 친구 추가/삭제 등의 일반적인 작업 처리

### 데이터 모델
- 위치 정보 캐시
  - key : 사용자 ID
  - value : 위도, 경도, 시각 정보
- 위치 정보 저장에 DB를 사용하지 않는 이유
  - 사용자의 **현재 위치**만 사용
  - 사용자 위치를 가장 최신 하나만 사용하므로 읽기/쓰기 연산이 빠른 레디스가 적합함
  - 위치 정보에 대해서 영속성을 보장할 필요가 없음.
- 위치 이동 이력 DB
  - 위치 이동 이력에 대해서는 막대한 쓰기 연산 부하를 감당할 수 있어야 함
  - 카산드라 DB가 목적에 부합할 수 있음
  - RDB는 한 대에 보관하기에 너무 많아서 샤딩이 필요할 수 있음.

## 3단계 - 상세 설계
### 중요 구성요소별 규모 확장성 - API 서버
- 무상태 서비스이므로 쉽게 확장 가능

### 중요 구성요소별 규모 확장성 - 웹소켓 서버
- 규모를 자동으로 늘리는 건 어렵지 않음.
- 유상태 서버라 기존 서버 제거에는 주의가 필요함
- 서버 노드를 제거하기 전 기존 연결부터 종료해야함
  - 로드밸런서가 인식하는 노드 상태를 '연결 종료 중(draining)'으로 변경
  - 새로운 연결은 만들어지지 않음
  - 모든 연결이 종료되면 서버 제거

### 중요 구성요소별 규모 확장성 - 클라이언트 초기화
- 모바일 클라이언트는 웹소켓 서버 하나와 지속성 웹소켓 연결을 맺음.
- 위치 정보를 받은 웹소켓의 동작
  - 위치 정보 캐시에 보관된 사용자 위치 갱신
  - 위치 정보는 계산에 사용하기 위해 변수에 저장
  - DB에서 사용자의 모든 친구 정보를 가져옴
  - 위치 정보 캐시에 일괄 요청을 보내 모든 친구의 위치를 한 번에 가져옴.
    - TTL은 비활성화 타임아웃 시간(intactivity timeout period)와 동일한 값이므로, 모든 값은 활성 사용자의 값임
  - 캐시가 돌려준 각각의 위치 정보에 대해 거리를 계산
    - 거리 반경 내인 정보만 클라에게 반환
  - 웹소켓 서버는 각 친구의 레디스 서버 펍/섭 채널을 구독
    - 레디스 펍/섭 서버는 저렴해서 활성 여부와 관계 없이 모든 친구를 구독 가능
  - 사용자의 현재 위치를 레디스 펍/섭 서버의 전용 채널을 통해 모든 친구에게 전송

### 중요 구성요소별 규모 확장성 - 사용자 DB
- 두가지 종류의 데이터가 보관
  - 사용자 상세 정보
  - 친구 관계 데이터
- 사용자 ID를 기준으로 데이터를 샤딩

### 중요 구성요소별 규모 확장성 - 위치 정보 캐시
- 활성 상태 유저 위치 정보를 캐시하기 위해 레디스 사용
- 각 항목의 키에는 TTL을 설정
- 위치 정보 갱신 시 TTL도 초기화
- 위치 정보 보관에 100바이트가 필요하다고 가정하면 천 만명의 유저 정보도 한 대의 레디스 서버로도 보관 가능
  - 수GB 정도 메모리만 있으면 됨
- 천 만명의 활성 유저가 30초마다 위치 정보를 갱신하면 초당 334k의 연산을 하게 되어 부담될 수 있음.
- 사용자 ID를 기준으로 샤딩 가능

### 중요 구성요소별 규모 확장성 - 레디스 펍/섭 서버
- 모든 온라인 친구에게 보내는 위치 변경 내역 메시지 라우팅 계층으로 활용
- 채널을 만드는 비용이 저렴해서 레디스 펍/섭 서버를 사용함
- 구독자가 없는 채널로 전송된 메시지는 버려지는데, 서버에 가해지는 부하는 거의 없음
- 채널 하나를 유지하기 위해 구독자 관계를 추적하기 위한 해시 테이블과 연결 리스트가 필요함
  - 아주 소량의 메모리만 사용
  - 오프라인 사용자라 변경이 없는 채널은 생성 이후 CPU 자원을 전혀 사용하지 않음
    - 모든 사용자에게 채널 하나씩 부여
    - 온/오프라인에 상관 없이 모든 구독 관계를 설정
    - 이렇게 하면 활성 여부와 상관 없이 구현이 간단해짐
    - 다만, 더 많은 메모리를 사용하게 됨
      - 메모리가 병목 될 가능성은 낮음

### 중요 구성요소별 규모 확장성 - 얼마나 많은 레디스 펍/섭 서버가 필요한가?
- 메모리 사용량
  - 사용자에게 채널 하나씩 할당 시 전체 채널 수는 1억(10억 사용자의 10%)
  - 사용자의 활성 친구 중 100명이 주변 친구 기능을 사용한다고 가정
  - 구독자 한 명을 추적하기 위해 내부 해시 테이블과 연결 리스트에 20바이트 상당의 포인터들을 저장한다고 가정
  - 모든 채널 저장에 200GB의 메모리 필요
    - 1억 * 20바이트 * 100명의 친구 / 10^9 = 200GB
  - 100GB 메모리 설치 가능한 최신 서버의 경우 모든 채널 보관에 레디스 펍/섭 서버 두 대면 해결됨
- CPU 사용량
  - 펍/섭 서버가 구독자에게 전송해야하는 위치 정보 업데이트 양은 초당 1400만 건에 달함
  - 보수적으로 서버 한 대가 10만개의 구독자 수를 감당한다고 하면 140대의 레디스 서버가 필요함
    - 1400만 / 100_000 = 140대
  - 레디스 펍/섭 서버의 병목은 메모리가 아니라 CPU 사용량임
  - 분산 레디스 펍/섭 클러스터가 필요함
  
### 중요 구성요소별 규모 확장성 - 분산 레디스 펍/섭 서버 클러스터
- 모든 채널이 독립적임
  - 사용자 ID를 기준으로 샤딩
  - 서비스 탐색(service discovery) 컴포넌트를 도입
    - etcd, 주키퍼 등
    - 가용한 서버 목록을 유지하는 기능 및 해당 목록을 갱신하는 데 필요한 UI나 API
      - 설정(configuration) 데이터를 보관하기 위한 소규모 key-value 저장소
    - 클라이언트(웹소켓 서버)로 하여금 '값'에 명시된 레디스 펍/섭 서버에서 발생한 변경 내역을 구독할 수 있는 기능
    - key에 달린 value에는 활성 상태의 모든 레디스 펍/섭 서버로 구성된 해시 링에 보관
      - 1권 5장에 나왔던 안정 해시 설계 부분 참고
  - 웹소켓 서버가 특정 사용자 채널에 위치 정보 변경 내역을 발행하는 과정
    - 웹소켓 서버는 해시 링을 참조해 메시지를 발행할 펍/섭 서버를 선정
      - 서비스 탐색 컴포넌트에 정보가 저장되어 있는데, 성능을 높이고 싶으면 해시 링 사본을 웹소켓 서버에 캐시할 수 있음. 다만, 링 원본에 구독 관계를 설정해 사본의 상태를 항상 원본과 동일하게 해야함
    - 웹소켓 서버는 해당 서버가 관리하는 사용자 채널에 위치 정보 변경 내역을 발행
    - 구독한 채널이 존재하는 레디스 펍/섭 서버를 찾는 과정도 동일함

### 중요 구성요소별 규모 확장성 - 레디스 펍/섭 서버 클러스터의 규모 확장 고려사항
- 레디스 펍/섭 서버 클러스터의 속성
  - 채널에 전송되는 메시지는 메모리나 디스크에 지속적으로 보관되지 않음.
    - 모든 구독자에게 전송 시 삭제됨.(구독자가 없으면 바로 삭제)
    - 채널을 통해 처리되는 데이터가 무상태라고 할 수 있음
  - 채널에 대한 상태 정보는 보관함
    - 각 채널의 구독자 목록은 그 상태 정보의 핵심임
    - 특정 채널을 담당하던 서버가 교체되거나 해시 링에서 제거되는 등의 경우 다른 서버로 이동시키고 해당 채널의 모든 구독자에게 이를 알려야 함
    - 이런 이유로 펍/섭은 유상태 서버임
- 레디스 펍/섭 서버 클러스터는 **유상태 서버 클러스터**로 처리하는 것이 옳음
  - 유상태 서버 클러스터 규모를 늘리거나 줄이는 것은 운영 부담과 위험이 큰 작업임
  - 혼잡한 시간대 트래픽을 무리 없이 감당하고 불필요한 크기 변화를 피하도록 어느 정도 여유를 두고 오버 프로비저닝(over provi-sioning)하는 것이 좋음
  - 불가피하게 규모를 늘릴 경우 주의할 문제
    - 클러스터 크기 조정 시 많은 채널이 해시 링 위의 다른 서버로 이동할 수 있음
      - 링이 갱신되면 엄청난 재구독(resubscription) 부하가 발생할 수 있음
    - 재구독 처리로 인해 새로운 위치 정보 변경 메시지가 누락될 수 있음
      - 어느 정도는 허용할 수 있지만, 최소화할 수 있어야 함
    - 서비스의 상태가 불안정해질 수 있으므로 부하가 가장 낮은 시간에 해야 함
- 클러스터 크기 조정
  - 새로운 링 크기를 계산
    - 크기가 늘어날 경우 새 서버를 준비
  - 해시 링의 키에 달린 값을 새로운 내용으로 갱신
  - 대시보드를 모니터링 해 웹소켓 클러스터의 CPU 사용량이 어느정도 튀는 것이 보야아 함.

### 중요 구성요소별 규모 확장성 - 운영 고려사항
- 기존 레디스 펍/섭 서버를 새 서버로 교체할 때 발생하는 문제는 클러스터 크기 조정보다 훨씬 낮음
  - 채널의 대규모 이동 사태가 없음
- 다만, 서버 장애는 생기게 되어 있고 그런 서버는 일상적으로 교체해야 함
- 펍/섭 서버 장애 시 모니터링 소프트웨어는 엔지니어에게 경보를 발송해야함
- 노드 교체 시 모든 웹소켓 서버에 통지하고 새 채널 구독을 알려야 함.
- 웹소켓 서버로부터 통지를 받으면 모든 채널을 해시 링과 대조해 새 서버로 구독 관계 설정을 해야하는지 검토함

### 친구 추가/삭제
- 사용자가 친구를 추가하거나 제거하면 웹소켓 서버의 연결 핸들러에게 알려야 함
- 웹소켓 서버로 새 친구의 펍/섭 채널을 구독하라는 메시지를 보내야 함
- 친구 삭제도 마찬가지임

### 친구가 많은 사용자
- 친구가 많은 사용자가 시스템 성능 문제를 야기할 수 있는가?
- 최대 친구 수에 상한이 있다고 가정(페이스북은 5000명)
- 친구 관계는 양방향임
  - 100만 팔로워는 존재할 수 없음
- 친구 구독에 필요한 펍/섭 구독 관계는 여러 웹소켓 서버에 분산되어 있음.
  - 친구 위치 변경에 따른 부하는 여러 웹소켓 서버에 분할되어 있으므로 핫스팟 문제는 발생하지 않을 것임

### 주변의 임의 사용자
- 기존 설계를 최대한 활용할 경우 지오해시에 따라 구축된 펍/섭 채널 풀을 두는 것임
  - 지오해시의 각 격자에 채널을 하나씩 만듦.
  - 해당 격자 내의 모든 사용자가 해당 격자 채널을 구독하게 함.
  - 격자 경계 부근 사용자 처리를 위해 사용자가 위치한 지오해시뿐 아니라 주변 지오해시도 구독함.

### 펍/섭 외의 대안
- 레디스 펍/섭 외의 대안은 없는가?
- 얼랭(Erlang)은 유용한 해결책이 될 수 있음
  - 다만 얼랭은 사용자가 많지 않은 언어라서 좋은 프로그래머를 구하기 힘듦
  - 얼랭은 고도로 분산된 병렬 애플리케이션을 위해 고안된 프로그래밍 언어이자 런타임 환경임
  - 얼랭의 강력함은 경량 프로세스임
    - 얼랭 프로세스는 BEAM VM에서 실행되는 개체(entity)임
    - 프로세스 생성 비용이 리눅스 프로세스 생성에 비해 엄청 저렴함
    - 가장 작은 프로세스는 300바이트의 메모리만 사용함
    - 최신 서버 한 대로 수백만 프로세스를 실행 가능함
    - 아무 작업도 하지 않는 프로세스는 CPU 자원을 전혀 소모하지 않음
  - 얼랭을 사용해 웹소켓 서비스를 구현하고, 레디스 펍/섭 서버에서 각 사용자는 얼랭 프로세스 하나로 대체 가능
  - 얼랭/OTP는 구독 기능을 내장하고 있어서 레디스 펍/섭 서버를 대체할 수 있음

## 4단계 - 마무리
- 핵심 컴포넌트
  - 웹 소켓
    - 클라이언트와 서버 사이의 실시간 통신
  - 레디스
    - 위치 데이터의 빠른 읽기/쓰기 지원
  - 레디스 펍/섭
    - 한 사용자의 위치 정보 변경 내역을 모든 온라인 친구에게 전달하는 라우팅 계층


### Q) 웹소켓(유상태)과 RESTful 서버(무상태) 두 개를 사용함. 무상태와 유상태 각각을 로드밸런서에서 처리하는 방법은?
- 유상태에서는 세션을 유지해야 하므로 로드밸런서에서 세션을 유지해야 함
  - 고정 스티키 세션 방식

### Q) 무상태와 유상태 두 가지가 있을 경우 로드밸런서를 두 개 써야할까?
- 하나의 로드밸런서로도 가능함.
  - 각각의 서버 클러스터에 따라 설정을 다르게하면 하나의 로드밸런서로 처리할 수 있음

### 레디스 펍/섭
- 펍/섭의 경우 메시지 손실이 일어날 수 있음
  - 발행 이후 수신도 확인하지 않고 삭제함.
- 손실 방지 방법
  - 메시지를 우선 큐에 저장해두고, 해당 큐에서 사용자가 가져가는 방식을 사용하면 될 것 같음
- 비활성화 상태의 유저는 pub/sub 채널 유지를 위해 필요한 메모리 크기는 소량이고, CPU와 I/O는 미사용
  - 메모리에서만 소모되는 자원임
  - 디스크에 저장되지 않기에 I/O는 일어나지 않음

### 카프카 vs 레디스 펍/섭
- 카프카
  - 카프카는 디스크에 기록함
  - 조금더 안정적으로 처리가 가능함
  - what?
    - 메시지를 발행하고 구독하는 역할을 하는 메시지 큐
  - why?
    - 비동기로 처리할 수 있는 작업에 대해서 메시지 형태로 관리하기 위함
    - 메시지 큐를 통해 서비스 간의 결합도를 낮추기 위함
      - 펍/섭 형태로 이루어져 있으므로 퍼블리셔와 서브스크라이버 사이의 결합도가 낮음
    - 수신 여부 확인 등의 메시지 손실 방지를 하기 편하다
  - how?
    - 멀티스레드 방식
      - 멀티스레드이므로 병렬로 빠르게 처리 가능
    - 디스크에 저장함
    - 메시지를 polling 하는 방식
  - problem?
    - 단순하게 펍/섭 형태 자체만 구현하려고 할 경우 너무 볼륨이 커질 수 있음
      - 카프카를 사용하기 위해 비즈니스 로직이나 DevOps 관련 준비가 필요함
    - 채널 생성에 비용이 크다
- 레디스 펍/섭
  - 레디스 펍/섭은 메모리에만 저장함
  - 레디스 펍/섭은 메시지 손실이 발생할 수 있음
  - what?
    - 메시지를 발행하고 구독하는 역할을 하는 메시지 큐
  - why?
    - 비동기로 처리할 수 있는 작업에 대해서 메시지 형태로 관리하기 위함
    - 메시지 큐를 통해 서비스 간의 결합도를 낮추기 위함
      - 펍/섭 형태로 이루어져 있으므로 퍼블리셔와 서브스크라이버 사이의 결합도가 낮음
    - 비활성 펍/섭에 대해서 CPU 사용과 I/O가 없음
  - how?
    - 싱글스레드 방식
      - 메시지 발행도 싱글 스레드이므로 느릴 수 있음
    - 메시지를 push 하는 방식
      - 속도가 빠름
  - problem?
    - 메시지의 안정성을 보장하기 어렵다
    - 메모리에 채널이 생성된다는 점에서 가장 비싼 메모리 사용률이 올라가는 점
      - 펍/섭만이 아닌 다른 용도로도 쓰일 수 있는 값 비싼 메모리를 사용하게 됨
    - TTL 설정 등을 잘하지 않을 경우 낭비되는 메모리가 많을 것 같음
      - monitor 명령어 등을 통해서 TTL 설정과 낭비되는 메모리를 관리해줘야 함
      - 모니터링 툴로 Redis 메모리 사용량 등도 계속 관리해야 함

### redis로 인한 네이버와 쿠팡에서 발생한 장애는?
- redis는 싱글 스레드
  - keys 명령어를 사용해 장애가 발생함.
- 쿠팡에서 모든 재고가 0이 되는 문제
  - key가 많아져서 int 최대 값을 넘어버림